{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(d, n_train, n_test):\n",
    "  x_train = np.random.randn(n_train, d)\n",
    "  x_test = np.random.randn(n_test, d)\n",
    "  mean_train = np.empty(d)\n",
    "  norm_tab = np.empty(n_train)\n",
    "\n",
    "  norm_train = (np.sum(x_train * x_train, axis = 1) ** 0.5).reshape(n_train, 1)\n",
    "  x_train = (x_train / norm_train) * (d ** 0.5)\n",
    "      \n",
    "  norm_test = (np.sum(x_test * x_test, axis = 1) **0.5).reshape(n_test, 1)\n",
    "  x_test = (x_test / norm_test) * (d**0.5)\n",
    "\n",
    "  #On vérifie que notre data set soit uniforme (sur chaque dimension, le dataset a à peu près une moyenne de 0)\n",
    "  #On se rapproche de plus en plus de 0 lorsqu'on augmente n_train, les données étant mieux réparties\n",
    "  mean_train = np.sum(x_train, axis = 0) / n_train\n",
    "      \n",
    "  #On vérifie que chaque élément de notre dataset ait une norme de sqrt(d)\n",
    "  norm_tab = np.sum(x_train * x_train, axis = 1) ** 0.5\n",
    "      \n",
    "  tau = 0.\n",
    "  noise_level = tau * tau\n",
    "  #On veut E(noise) == 0 et E(noise^2) == tau^2\n",
    "  noise_train = np.random.randn(n_train, 1) * noise_level\n",
    "  noise_test = np.random.randn(n_test, 1) * noise_level\n",
    "\n",
    "  F1 = 1\n",
    "\n",
    "  sample_params = np.random.randn(d, 1)\n",
    "      \n",
    "  norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "  sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "  y_train = np.dot(x_train, sample_params) + noise_train\n",
    "  y_test = np.dot(x_test, sample_params) + noise_test\n",
    "\n",
    "  return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return Sigmoid.function(x) * Sigmoid.function(-x)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.tanh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh.function(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return x >= 0\n",
    "\n",
    "class Linear :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return np.ones((x.shape[0], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return np.mean(np.sum((y_hat.T - y_real.T)**2, axis = 0))    \n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/y_real.shape[0]) * (y_hat - y_real)\n",
    "    \n",
    "class reg_MSE:\n",
    "    def __init__(self, weights, lam):\n",
    "        self.weights = weights\n",
    "        self.lam = lam\n",
    "        \n",
    "    def loss(self, y_real, y_hat):\n",
    "        loss = np.mean(np.sum((y_hat.T - y_real.T)**2, axis = 0))\n",
    "        reg = (np.sum(np.square(self.weights[1])) + np.sum(np.square(self.weights[2])))*(self.lam * self.weights[1].shape[1]/self.weights[1].shape[0])\n",
    "        return loss + reg\n",
    "        \n",
    "    def gradient(self, y_real, y_hat):\n",
    "        loss_grad = (2/y_real.shape[0]) * (y_hat - y_real)\n",
    "        reg_grad = (self.lam * self.weights[1].shape[1]/self.weights[1].shape[0]) * (2*(np.sum(self.weights[1]) + np.sum(self.weights[2])))\n",
    "        return loss_grad + reg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, d, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "                \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "                \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(1, self.nb_layers):\n",
    "            self.weights[i] = np.random.randn(self.dimensions[i - 1], self.dimensions[i]) / np.sqrt(self.dimensions[i - 1])\n",
    "            self.bias[i] = np.zeros(self.dimensions[i])\n",
    "         \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \n",
    "        return : z contient les paramètres avant que l'on applique l'activation function\n",
    "        return : a contient les paramètres après que l'on applique l'activation function\n",
    "        \"\"\"\n",
    "        self.z = {}\n",
    "        self.a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        for i in range(1, self.nb_layers):\n",
    "            self.z[i + 1] = np.dot(self.a[i], self.weights[i]) #+ self.bias[i] #Z = XW + b\n",
    "            self.a[i + 1] = self.activations[i + 1].function(self.z[i + 1])\n",
    "            \n",
    "        return self.z, self.a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, y_real):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(y_real, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = np.dot(a[self.nb_layers - 1].T, delta)\n",
    "     \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'hidden layer\n",
    "        delta = np.dot(delta, self.weights[2].T) * self.activations[2].gradient(z[2])\n",
    "        partial_deriv = np.dot(a[1].T, delta) \n",
    "        update_parameters[1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        #self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_real, x_test, y_test, loss, nb_iterations = 100, batch_size = 100, learning_rate1 = 0, learning_rate2 = 0.3):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "            \n",
    "        loss_tab = []\n",
    "        \n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            _, a_train = self.forward_pass(x)\n",
    "            _, a_test = self.forward_pass(x_test)\n",
    "            loss_tab.append({\n",
    "                \"train_mse\":  self.loss_function.loss(y_real, a_train[self.nb_layers]),\n",
    "                \"test_mse\":  self.loss_function.loss(y_test, a_test[self.nb_layers]),\n",
    "            })\n",
    "            #if(i % 50) == 0:\n",
    "                #print(f\"Loss at Iteration {i} for first batch is {loss_tab[-1]}\")\n",
    "        return loss_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = 100\n",
    "n_train = 300\n",
    "n_test = 300\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "nn = Network(200, Linear, Linear)\n",
    "config = dict(\n",
    "        nb_iterations=30000, \n",
    "        batch_size=300, \n",
    "        learning_rate1=0, \n",
    "        learning_rate2=0.001\n",
    "    )\n",
    "\n",
    "loss_func = MSE #reg_MSE(nn.weights, 10**(-8))\n",
    "history = nn.fit(x_train, y_train, x_test, y_test, loss_func, **config)\n",
    "\n",
    "test_prediction = nn.predict(x_test)\n",
    "print(test_prediction)\n",
    "\n",
    "#print(y_test[0])\n",
    "\n",
    "beta_calc = np.dot(np.dot(np.linalg.inv(np.dot(x_train.T, x_train)), x_train.T), y_train)\n",
    "real_test_prediction = np.dot(beta_calc.T, x_test.T)\n",
    "print(\"#########################\")\n",
    "print(real_test_prediction)\n",
    "print(MSE.loss(test_prediction, real_test_prediction.T))\n",
    "\n",
    "df = pd.DataFrame(history)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(-0.1, 3)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress 0.00%, N = 30.0\n",
      "0.8542862104331171\n",
      "0.8967391904840251\n",
      "0.9497115287067911\n",
      "1.0392842084544347\n",
      "0.8831442401791079\n",
      "progress 3.45%, N = 60.0\n",
      "0.7571361750227904\n",
      "0.8246886921488833\n",
      "0.7734263284795336\n",
      "0.6624259748618492\n",
      "0.813175191357202\n",
      "progress 6.90%, N = 90.00000000000001\n",
      "0.7787006100643055\n",
      "0.6474951494281903\n",
      "0.6366742435480429\n",
      "0.7574448106447536\n",
      "0.775480919574796\n",
      "progress 10.34%, N = 120.0\n",
      "0.6350622619737911\n",
      "0.5809118706855717\n",
      "0.6046093944791155\n",
      "0.747033034443956\n",
      "0.589818306066328\n",
      "progress 13.79%, N = 150.0\n",
      "0.59167955919113\n",
      "0.8745859016316172\n",
      "0.6002086620915427\n",
      "0.6893278321992368\n",
      "0.6958492526878195\n",
      "progress 17.24%, N = 180.0\n",
      "0.4732663170359325\n",
      "0.5039858441391132\n",
      "0.5447464269100382\n",
      "0.6705111388738639\n",
      "0.7018446083273974\n",
      "progress 20.69%, N = 210.00000000000003\n",
      "0.5324037301811474\n",
      "0.6264788676828987\n",
      "0.7696192718251087\n",
      "0.7659844665972816\n",
      "0.4835268968972456\n",
      "progress 24.14%, N = 240.0\n",
      "0.7626177097652015\n",
      "0.7703123649382971\n",
      "0.7236922920063781\n",
      "1.0055747967506437\n",
      "0.994924662582982\n",
      "progress 27.59%, N = 270.0\n",
      "2.1800572707009738\n",
      "1.0052474258681687\n",
      "2.6667999412099452\n",
      "1.4810022261957816\n",
      "1.4655866500298829\n",
      "progress 31.03%, N = 300.0\n",
      "6.254734509678266\n",
      "6.135434039859415\n",
      "9.973731252735858\n",
      "8.545903632128097\n",
      "5.071959865093317\n",
      "progress 34.48%, N = 330.0\n",
      "1.598211620747458\n",
      "1.4651215381471403\n",
      "1.5903523120031446\n",
      "0.7748409742777295\n",
      "1.2352324788174625\n",
      "progress 37.93%, N = 360.00000000000006\n",
      "0.7755594621481001\n",
      "0.837232108940045\n",
      "0.7908099454496003\n",
      "0.9270971923174898\n",
      "0.6064367857441365\n",
      "progress 41.38%, N = 390.00000000000006\n",
      "0.4428599945990323\n",
      "0.3124787271209724\n",
      "0.5025605548924127\n",
      "0.6188339825897395\n",
      "0.43529343809419674\n",
      "progress 44.83%, N = 420.00000000000006\n",
      "0.36842246057243433\n",
      "0.32201168412541187\n",
      "0.41984348567536967\n",
      "0.365039642703104\n",
      "0.33346899080373266\n",
      "progress 48.28%, N = 450.00000000000006\n",
      "0.3673650096927165\n",
      "0.41537462276446707\n",
      "0.33872259481992967\n",
      "0.272226487145509\n",
      "0.3369996818183018\n",
      "progress 51.72%, N = 480.0\n",
      "0.24895181407599684\n",
      "0.2055207338262377\n",
      "0.2533667167382586\n",
      "0.26438899058058407\n",
      "0.24806390759561406\n",
      "progress 55.17%, N = 510.00000000000006\n",
      "0.17586469924334586\n",
      "0.21607706741403818\n",
      "0.2618518682587261\n",
      "0.18203547130686654\n",
      "0.21379680960348169\n",
      "progress 58.62%, N = 540.0000000000001\n",
      "0.2154633507292087\n",
      "0.20106363200038155\n",
      "0.23626280391266097\n",
      "0.19792271627185792\n",
      "0.18513118235527343\n",
      "progress 62.07%, N = 570.0\n",
      "0.18517149930048235\n",
      "0.18939213389364762\n",
      "0.16387615876563968\n",
      "0.15366730197862574\n",
      "0.16828602160913766\n",
      "progress 65.52%, N = 600.0\n",
      "0.18176855035897577\n",
      "0.1843233894438059\n",
      "0.20373678840347484\n",
      "0.20482809142467404\n",
      "0.17006845395205966\n",
      "progress 68.97%, N = 630.0\n",
      "0.1525394015017021\n",
      "0.16375419323118462\n",
      "0.12962563475654054\n",
      "0.1542300376767951\n",
      "0.16047900888931035\n",
      "progress 72.41%, N = 660.0\n",
      "0.10873268609062076\n",
      "0.1364657455605708\n",
      "0.12993079654916345\n",
      "0.17171401847723852\n",
      "0.16439700886960096\n",
      "progress 75.86%, N = 690.0000000000001\n",
      "0.11515964345994871\n",
      "0.1329468806432206\n",
      "0.14009168291332058\n",
      "0.11144382381822138\n",
      "0.1347279046688408\n",
      "progress 79.31%, N = 720.0000000000001\n",
      "0.10541106314013511\n",
      "0.10374107330769973\n",
      "0.14628368133517886\n",
      "0.11454740394515947\n",
      "0.18030119732316188\n",
      "progress 82.76%, N = 750.0000000000001\n",
      "0.11153813793296019\n",
      "0.09800726280803161\n",
      "0.10893462036707334\n",
      "0.12044033869335857\n",
      "0.10616431681870518\n",
      "progress 86.21%, N = 780.0\n",
      "0.10788831752206751\n",
      "0.1102987243069368\n",
      "0.11893767653144768\n",
      "0.12098608148861743\n",
      "0.10907560118196384\n",
      "progress 89.66%, N = 810.0\n",
      "0.12685936182995902\n",
      "0.10513215731589168\n",
      "0.10885869241824679\n",
      "0.11700350206393056\n",
      "0.09473945886649794\n",
      "progress 93.10%, N = 840.0000000000001\n",
      "0.09632790298847464\n",
      "0.11007904795293384\n",
      "0.10678297418716738\n",
      "0.1186472725491874\n",
      "0.09818243390268193\n",
      "progress 96.55%, N = 870.0000000000001\n",
      "0.11062204988430858\n",
      "0.08745778062679217\n",
      "0.12517335795250778\n",
      "0.09061399116443394\n",
      "0.11568359224710349\n"
     ]
    }
   ],
   "source": [
    "#With the formula\n",
    "d=100\n",
    "n_train = 300\n",
    "n_test = 300\n",
    "\n",
    "N_SPACE = np.arange(0.1, 3, 0.1)\n",
    "l_f = []\n",
    "for i, N in enumerate(N_SPACE):\n",
    "    N = n_train * N\n",
    "    print(f\"progress {100*i/len(N_SPACE):.2f}%, N = {N}\")\n",
    "    l0_f = []\n",
    "    for _ in range(5):\n",
    "        x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "        \n",
    "        theta = np.random.randn(d, int(N)) / (d**0.5)\n",
    "        x_train = Relu.function(np.dot(x_train, theta))\n",
    "        x_test = Relu.function(np.dot(x_test, theta))\n",
    "        \n",
    "        beta_calc = np.dot(np.dot(np.linalg.inv(np.dot(x_train.T, x_train) + (10**(-3) * N/d) * np.eye(x_train.shape[1], x_train.shape[1])), x_train.T), y_train)\n",
    "        test_prediction = np.dot(beta_calc.T, x_test.T)\n",
    "        test_error = MSE.loss(y_test, test_prediction.T)\n",
    "        l0_f.append(test_error)\n",
    "        print(test_error)\n",
    "    l0_f = np.array(l0_f)\n",
    "    l_f.append({\"mean\": l0_f.mean(), \"std\": l0_f.std()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(l_f)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(-0.1, 2)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(l_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the neural network\n",
    "\n",
    "#d = 100\n",
    "n_train = 100\n",
    "n_test = 30\n",
    "\n",
    "N_SPACE = np.arange(0.1, 5, 0.1)\n",
    "\n",
    "l = []\n",
    "\n",
    "for i, N in enumerate(N_SPACE):\n",
    "    d = int(N*n_train)\n",
    "    N = d\n",
    "    print(f\"progress {100*i/len(N_SPACE):.2f}%, N = {N}\")\n",
    "    l0 = []\n",
    "    for _ in range(5):\n",
    "        x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "        x_train = Relu.function(x_train)\n",
    "        x_test = Relu.function(x_test)\n",
    "        nn = Network(N, d, Linear, Linear)\n",
    "        config = dict(\n",
    "            nb_iterations=500, \n",
    "            batch_size=n_train, \n",
    "            learning_rate1=0., \n",
    "            learning_rate2=0.01\n",
    "        )        \n",
    "        loss_func = reg_MSE(nn.weights, 10**(-8))\n",
    "        history = nn.fit(x_train, y_train, x_test, y_test, loss_func, **config)\n",
    "        test_prediction = nn.predict(x_test)\n",
    "        final_test_error = nn.loss_function.loss(y_test, test_prediction)\n",
    "        l0.append(final_test_error)\n",
    "        print(final_test_error)\n",
    "    l0 = np.array(l0)\n",
    "    l.append({\"mean\": l0.mean(), \"std\": l0.std()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(l)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(-0.1, 3)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param=0.1\n",
    "\n",
    "n_train = 100\n",
    "d = int(param * n_train)\n",
    "x_train, y_train, x_test, y_test = get_dataset(d, n_train, 30)\n",
    "x_train = Relu.function(x_train)\n",
    "x_test = Relu.function(x_test)\n",
    "\n",
    "\n",
    "nn = Network(d, d, Linear, Linear)\n",
    "config = dict(\n",
    "    nb_iterations=500, \n",
    "    batch_size=n_train, \n",
    "    learning_rate1=0., \n",
    "    learning_rate2=0.0001\n",
    ")\n",
    "print(x_train.shape)\n",
    "print(nn.dimensions)\n",
    "print(nn.weights[1].shape)\n",
    "loss_func = reg_MSE(nn.weights, 10**(-8))\n",
    "history = nn.fit(x_train, y_train, x_test, y_test, loss_func, **config)\n",
    "print(history[-1])\n",
    "test_prediction = nn.predict(x_test)\n",
    "final_test_error = nn.loss_function.loss(y_test, test_prediction)\n",
    "print(final_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.weights[1].shape)\n",
    "beta_nn = np.dot(nn.weights[1], nn.weights[2])\n",
    "beta_calc = np.dot(np.dot(np.linalg.inv(np.dot(x_test.T, x_test) + 0.01 * np.eye(x_test.shape[1], x_test.shape[1])), x_test.T), y_test)\n",
    "print(beta_nn)\n",
    "print(beta_calc)\n",
    "print(mean_squared_error(beta_calc, beta_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
