{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(d, n_train, n_test):\n",
    "  x_train = np.random.randn(n_train, d)\n",
    "  x_test = np.random.randn(n_test, d)\n",
    "  mean_train = np.empty(d)\n",
    "  norm_tab = np.empty(n_train)\n",
    "\n",
    "  norm_train = (np.sum(x_train * x_train, axis = 1) ** 0.5).reshape(n_train, 1)\n",
    "  x_train = (x_train / norm_train) * (d ** 0.5)\n",
    "      \n",
    "  norm_test = (np.sum(x_test * x_test, axis = 1) **0.5).reshape(n_test, 1)\n",
    "  x_test = (x_test / norm_test) * (d**0.5)\n",
    "\n",
    "  #On vérifie que notre data set soit uniforme (sur chaque dimension, le dataset a à peu près une moyenne de 0)\n",
    "  #On se rapproche de plus en plus de 0 lorsqu'on augmente n_train, les données étant mieux réparties\n",
    "  mean_train = np.sum(x_train, axis = 0) / n_train\n",
    "      \n",
    "  #On vérifie que chaque élément de notre dataset ait une norme de sqrt(d)\n",
    "  norm_tab = np.sum(x_train * x_train, axis = 1) ** 0.5\n",
    "      \n",
    "  tau = 0.1 \n",
    "  noise_level = tau * tau\n",
    "\n",
    "  F1 = 1\n",
    "\n",
    "  sample_params = np.random.randn(d, 1)\n",
    "      \n",
    "  norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "  sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "  #On veut E(noise) == 0 et E(noise^2) == tau^2\n",
    "  noise_train = np.random.randn(n_train, 1) * noise_level\n",
    "  noise_test = np.random.randn(n_test, 1) * noise_level\n",
    "\n",
    "  y_train = np.dot(x_train, sample_params) + noise_train\n",
    "  y_test = np.dot(x_test, sample_params) + noise_test\n",
    "\n",
    "  return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return Sigmoid.function(x) * Sigmoid.function(-x)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.tanh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh.function(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return x >= 0\n",
    "\n",
    "class Linear :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return np.ones((x.shape[0], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return np.mean(np.sum((y_hat.T - y_real.T)**2, axis = 0))    \n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/y_real.shape[0]) * (y_hat - y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "                \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(1, self.nb_layers):\n",
    "            self.weights[i] = np.random.randn(self.dimensions[i - 1], self.dimensions[i]) / np.sqrt(self.dimensions[i - 1])\n",
    "            self.bias[i] = np.zeros(self.dimensions[i])\n",
    "         \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \n",
    "        return : z contient les paramètres avant que l'on applique l'activation function\n",
    "        return : a contient les paramètres après que l'on applique l'activation function\n",
    "        \"\"\"\n",
    "        z = {}\n",
    "        a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        for i in range(1, self.nb_layers):\n",
    "            z[i + 1] = np.dot(a[i], self.weights[i]) + self.bias[i] #Z = XW + b\n",
    "            a[i + 1] = self.activations[i + 1].function(z[i + 1])\n",
    "            \n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, y_real):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(y_real, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = np.dot(a[self.nb_layers - 1].T, delta)\n",
    "     \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'hidden layer\n",
    "        delta = np.dot(delta, self.weights[2].T) * self.activations[2].gradient(z[2])\n",
    "        partial_deriv = np.dot(a[1].T, delta) \n",
    "        update_parameters[1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_real, x_test, y_test, loss, nb_iterations = 100, batch_size = 100, learning_rate1 = 0, learning_rate2 = 0.3):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "            \n",
    "        loss_tab = []\n",
    "        \n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            _, a_train = self.forward_pass(x)\n",
    "            _, a_test = self.forward_pass(x_test)\n",
    "            loss_tab.append({\n",
    "                \"train_mse\":  self.loss_function.loss(y_real, a_train[self.nb_layers]),\n",
    "                \"test_mse\":  self.loss_function.loss(y_test, a_test[self.nb_layers]),\n",
    "            })\n",
    "            if(i % 50) == 0:\n",
    "                print(f\"Loss at Iteration {i} for first batch is {loss_tab[-1]}\")\n",
    "        return loss_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress 0.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.0792390343906373, 'test_mse': 1.2811593202152824}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 1.079233505023173, 'test_mse': 1.2812200907248905}\n",
      "1.281279111932807\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.8642590885757186, 'test_mse': 0.8102901440412164}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.864233435218036, 'test_mse': 0.8103117264828766}\n",
      "0.8103329141492543\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.8913918375393456, 'test_mse': 1.2729417676058943}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.8912412017626166, 'test_mse': 1.2730917659801815}\n",
      "1.2732387562595868\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9053641278249475, 'test_mse': 0.8918790611178835}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.9053272786998201, 'test_mse': 0.8918353643027568}\n",
      "0.8917933156218111\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.0007793154442564, 'test_mse': 0.9452869744423557}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 1.0007432041046997, 'test_mse': 0.9454619900037187}\n",
      "0.9456321606746055\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.8627106730626695, 'test_mse': 1.0839748387793238}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.8627049890129773, 'test_mse': 1.083967317557926}\n",
      "1.0839600737223032\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.193332930340072, 'test_mse': 0.9988981488916395}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 1.1931865490706388, 'test_mse': 0.9991242060762947}\n",
      "0.9993449589477386\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9734312784163001, 'test_mse': 0.8963524127992862}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.9733913656210282, 'test_mse': 0.8962267910418038}\n",
      "0.8961052762135346\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.183400965420459, 'test_mse': 1.277945407361075}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 1.1832457111357175, 'test_mse': 1.277384618807794}\n",
      "1.276841944881569\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.024466675757223, 'test_mse': 1.0772754383555165}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 1.0238558896144632, 'test_mse': 1.0770578365028483}\n",
      "1.0768525270755898\n",
      "progress 8.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.13299940507066, 'test_mse': 1.2595629979840526}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.7244383557968805, 'test_mse': 1.15297002108358}\n",
      "1.204850528038819\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.9609660001799158, 'test_mse': 1.361686441932724}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.8965514297306346, 'test_mse': 0.8635991161638923}\n",
      "0.9193407953200893\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3640043214325086, 'test_mse': 1.3805725170803882}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.807349712269896, 'test_mse': 1.2552348982572363}\n",
      "1.2338018409196032\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.444181770866154, 'test_mse': 1.8941950001423624}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.7691971483564461, 'test_mse': 1.2075733532485158}\n",
      "1.1240885772173672\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.433521978584488, 'test_mse': 1.296270131511003}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.8746237418118201, 'test_mse': 1.1152702841128912}\n",
      "1.1368092345782168\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5829149520555268, 'test_mse': 1.4187607516791274}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.9735310142294067, 'test_mse': 1.3370328465195829}\n",
      "1.3855308712003163\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4177461307540498, 'test_mse': 1.1080812285343487}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.9365263347971845, 'test_mse': 0.9972448256680104}\n",
      "0.9837743870453631\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5828031694100095, 'test_mse': 1.680433264669557}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.8875513192230506, 'test_mse': 1.4020853801614017}\n",
      "1.3457788144837803\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.0871003620000288, 'test_mse': 1.5933354083990903}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.6753864346998852, 'test_mse': 1.4133722031953817}\n",
      "1.3628387658941656\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3530076654452845, 'test_mse': 1.607353023508803}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.9134344270444799, 'test_mse': 1.3995300880975565}\n",
      "1.3628552636438307\n",
      "progress 16.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9132750080595399, 'test_mse': 1.2829303458519112}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.5159675891486881, 'test_mse': 1.119144416910971}\n",
      "1.1244220396821836\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.470617544497823, 'test_mse': 1.3840015709257096}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.6674380538735006, 'test_mse': 1.1721553524177777}\n",
      "1.171779926143107\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5417757643524974, 'test_mse': 1.181579088899055}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.6659107046169193, 'test_mse': 0.9021809575457412}\n",
      "0.9444923898422419\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.763101680108977, 'test_mse': 1.2478029781132673}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.7942856012045741, 'test_mse': 0.952021109682594}\n",
      "0.9709516643921131\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4320454093978745, 'test_mse': 1.7933745932190497}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.5498799657670432, 'test_mse': 1.4794992631605237}\n",
      "1.435131967535162\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.262879722646164, 'test_mse': 1.2745703678149312}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.6705509111804677, 'test_mse': 1.12288235007909}\n",
      "1.1017063094873547\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2046524908557408, 'test_mse': 1.5712265108126768}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.6939976478444863, 'test_mse': 1.3831944251583355}\n",
      "1.3224927142089686\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4988256507693953, 'test_mse': 2.0485316897402357}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.657364932251944, 'test_mse': 1.6201398975691075}\n",
      "1.6238930997613712\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1511080160295428, 'test_mse': 1.1439842580013773}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.6475390977563148, 'test_mse': 1.1464278039509188}\n",
      "1.2374541891798365\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1062471634463302, 'test_mse': 1.149430669697149}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.6492963118205715, 'test_mse': 1.0420312150199775}\n",
      "1.091543457905512\n",
      "progress 25.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3256937485661282, 'test_mse': 1.450111813574729}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.41280047776866985, 'test_mse': 1.1345839848850812}\n",
      "1.0994458902351816\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1971335334693405, 'test_mse': 1.3226636296198617}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.3947340176094697, 'test_mse': 1.072884387221448}\n",
      "1.0749812164451273\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.635505437016744, 'test_mse': 1.2335534579892182}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.50440708456565, 'test_mse': 0.9181533310240486}\n",
      "0.9783900951357782\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3932578671776215, 'test_mse': 1.5073694331019223}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.5667795015700599, 'test_mse': 1.2731642151501081}\n",
      "1.2220547949378817\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.454864241565756, 'test_mse': 1.4698528739215826}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.5087227112511958, 'test_mse': 1.3900787650716737}\n",
      "1.4336193970235394\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.8649701255896298, 'test_mse': 1.437982742635986}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.723540437320857, 'test_mse': 1.2837145465498394}\n",
      "1.3114583463945961\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1790187926529052, 'test_mse': 1.2095998911302528}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.4650651842246032, 'test_mse': 1.0219674135256056}\n",
      "1.008861218506854\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.253176523444557, 'test_mse': 1.4106072992618153}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 50 for first batch is {'train_mse': 0.41669864279098734, 'test_mse': 1.2973341428306089}\n",
      "1.3399551468994946\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2704210507410756, 'test_mse': 1.405195326818297}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.49875842928992115, 'test_mse': 1.2475658734554853}\n",
      "1.2544410833982338\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.6632473776263705, 'test_mse': 1.7614172367496495}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.6323609730565388, 'test_mse': 1.2430329988380393}\n",
      "1.2450396199985678\n",
      "progress 33.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3993883388413808, 'test_mse': 1.3475423836494749}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.37205649640438665, 'test_mse': 1.1953096809153987}\n",
      "1.198748142513147\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2481448291473412, 'test_mse': 1.3283900504964345}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.4298504430767405, 'test_mse': 1.1757785912357634}\n",
      "1.1734058166160255\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.687918697688903, 'test_mse': 1.216646455204129}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.44968144077582567, 'test_mse': 1.0499008194627557}\n",
      "1.1046778772105597\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4968937265413464, 'test_mse': 1.3271578954794632}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.46440915975493746, 'test_mse': 1.126956040050369}\n",
      "1.1622991141930106\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1361211001081777, 'test_mse': 1.0508941446609934}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.2903744743700057, 'test_mse': 1.0154701249236846}\n",
      "1.0539403241303062\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.543237820536815, 'test_mse': 1.6882821901204363}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.358444830465134, 'test_mse': 1.3567238428877433}\n",
      "1.3611057741236616\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3004387868869511, 'test_mse': 1.3553989723497275}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.33514696724559784, 'test_mse': 1.142330823080067}\n",
      "1.1529072068282524\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3796369152259393, 'test_mse': 1.5511840097694423}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.3827399145399356, 'test_mse': 1.1291247073919586}\n",
      "1.1026422198426957\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9793276882940017, 'test_mse': 1.4774396037021378}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.3113571742092855, 'test_mse': 1.105620212983888}\n",
      "1.0578944889728132\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1382027328170883, 'test_mse': 1.21041323448879}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.24372353557293586, 'test_mse': 1.2354911016707009}\n",
      "1.3037550943608258\n",
      "progress 41.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.455138824853364, 'test_mse': 1.3273218391188326}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.3484563738120843, 'test_mse': 1.0955379773449272}\n",
      "1.0624582983236948\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1594765564637435, 'test_mse': 1.292690818121322}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.3252002421664202, 'test_mse': 1.1229227799801194}\n",
      "1.1197647886890063\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1923081559130682, 'test_mse': 0.9913023759510721}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.3053371722376041, 'test_mse': 0.8459104701620396}\n",
      "0.9112509434893521\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9422379580220809, 'test_mse': 1.113316926906725}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.16785696924455146, 'test_mse': 0.9378674141077461}\n",
      "0.9619737784537244\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.342119051222373, 'test_mse': 1.2993814852572259}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.29539552927379104, 'test_mse': 1.0328012069451602}\n",
      "1.0502378639696572\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.0122674148447728, 'test_mse': 1.3722531014329713}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.26180400396022735, 'test_mse': 1.1360775227147517}\n",
      "1.1235636560757336\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9883710977943667, 'test_mse': 1.7412599769143404}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.25167804270332567, 'test_mse': 1.667889847611782}\n",
      "1.7114488951630966\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3903318574263395, 'test_mse': 1.3114319515186532}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.3240896709163672, 'test_mse': 1.1049920439196679}\n",
      "1.104771965534682\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.041285157553959, 'test_mse': 1.45196425262784}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.25861574109505975, 'test_mse': 1.2897210408551247}\n",
      "1.3056220335467577\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1818774539869212, 'test_mse': 1.193148721315368}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.2445766068504789, 'test_mse': 1.0370955202454661}\n",
      "1.0553711754037878\n",
      "progress 50.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.288426050072619, 'test_mse': 1.1641016478882618}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.19198955987804583, 'test_mse': 1.0450353934486316}\n",
      "1.0648968631095184\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9463879985046336, 'test_mse': 1.463428163868304}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.1361447990014547, 'test_mse': 1.1892045495034935}\n",
      "1.1800023862409628\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4736899241368144, 'test_mse': 1.4679479753881381}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.202904930800326, 'test_mse': 1.1893938069710837}\n",
      "1.2151067710733718\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3145425878726922, 'test_mse': 1.3333976561958434}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.2511209330825573, 'test_mse': 0.9870768904670172}\n",
      "0.9709487680913732\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2547469768700383, 'test_mse': 1.256278116856144}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.2438391638179489, 'test_mse': 0.9575547822450676}\n",
      "0.9314208503212328\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2199950228056555, 'test_mse': 1.083857737843405}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.24394021821280126, 'test_mse': 1.0687486054011237}\n",
      "1.1477144449897525\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2447970202022154, 'test_mse': 1.5261888610641035}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.18089505678570228, 'test_mse': 1.3183927231466668}\n",
      "1.3473448122322946\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.6647600687757722, 'test_mse': 1.229225931326916}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.26201424643695864, 'test_mse': 1.1541244544996418}\n",
      "1.3358312197269762\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5822384112336076, 'test_mse': 1.6610871965269056}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.2377243205092238, 'test_mse': 1.358137953084218}\n",
      "1.3891637245247201\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1938642372982473, 'test_mse': 1.4657381255545423}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.18789411513541718, 'test_mse': 1.2497275267238515}\n",
      "1.198566378311183\n",
      "progress 58.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2108894683386777, 'test_mse': 1.3292598920905474}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.16410446904097578, 'test_mse': 0.9493818758815417}\n",
      "0.9377645758850575\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.072094116664936, 'test_mse': 1.4373135911275756}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.17662008158818387, 'test_mse': 1.2116761305958648}\n",
      "1.2151667393958463\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.01001103689998, 'test_mse': 1.3763653241507385}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.14558101079063587, 'test_mse': 1.28578670876527}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3135902237218062\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1033137965723474, 'test_mse': 1.3901270575192723}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.14526129328353005, 'test_mse': 1.1843980372128922}\n",
      "1.222705145851544\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9507424515198794, 'test_mse': 1.6317048825164435}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.14398167702003353, 'test_mse': 1.408566793704719}\n",
      "1.3962215955784147\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3624566237887419, 'test_mse': 1.4536633670895904}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.22941511286265878, 'test_mse': 1.194225027396581}\n",
      "1.1807976979135644\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4233464975095043, 'test_mse': 1.6087567539422158}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.18402862934134037, 'test_mse': 1.308248202619977}\n",
      "1.2853378784776746\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5319539359259895, 'test_mse': 1.7924682026914132}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.23035433506097736, 'test_mse': 1.2924418837625857}\n",
      "1.2462170343408696\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3564481463029847, 'test_mse': 1.2578514529229334}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.22009473294272855, 'test_mse': 1.1908112585380812}\n",
      "1.3047146742931899\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.018170962773446, 'test_mse': 1.2800899660923377}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.12250904864762198, 'test_mse': 1.134083249892833}\n",
      "1.1376982362474755\n",
      "progress 66.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.0134160110757937, 'test_mse': 1.2223318255516264}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.12036528226515934, 'test_mse': 1.0122750225752275}\n",
      "1.01681738297087\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4074235265096664, 'test_mse': 1.2035571455481384}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.1421783645367937, 'test_mse': 0.9210165593960581}\n",
      "0.9012980964428325\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2286013385525967, 'test_mse': 1.3010446031241003}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.154997646570145, 'test_mse': 1.0932073584875837}\n",
      "1.0772786332335407\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5913977259174388, 'test_mse': 1.4614858880888593}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.14721540601081673, 'test_mse': 1.307974379731369}\n",
      "1.3678148199544742\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2266950177133504, 'test_mse': 1.0644130345367129}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.1577554056341634, 'test_mse': 1.019586282902651}\n",
      "1.0511353215179322\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1715847490609506, 'test_mse': 1.2377429146536676}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.1418138370154922, 'test_mse': 1.015856482823944}\n",
      "1.0276938019379571\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.409285320841387, 'test_mse': 1.3340836144903598}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.1660366574225253, 'test_mse': 1.169028807007286}\n",
      "1.204417902789844\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.6558661878296204, 'test_mse': 1.1833977007945469}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.17178936269361794, 'test_mse': 0.9673295067603371}\n",
      "1.002511742835641\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2760189362959735, 'test_mse': 1.5488636513615257}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.1405486739560622, 'test_mse': 1.2554977335010296}\n",
      "1.2335894001697194\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3125193875053411, 'test_mse': 1.4178724042494724}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.14004427530997607, 'test_mse': 1.2285691642789838}\n",
      "1.213169584300443\n",
      "progress 75.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3575815459132219, 'test_mse': 1.1024880844775486}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.10929994357632712, 'test_mse': 0.8750901364847756}\n",
      "0.8899277347942457\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.242241551027175, 'test_mse': 1.3373937000735787}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.12239650956097604, 'test_mse': 1.0799522830115764}\n",
      "1.1054440570299067\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3151101316914369, 'test_mse': 1.1854047741587352}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.095632173853719, 'test_mse': 1.026918730753547}\n",
      "1.0513356995253151\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.36156360952597, 'test_mse': 1.5286325877802656}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.11148347098095679, 'test_mse': 1.2080142781862124}\n",
      "1.2313193610160638\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2825817534627348, 'test_mse': 1.5283363702281993}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.08929473996145748, 'test_mse': 1.0626517101018147}\n",
      "1.0463051786074162\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4918144281266472, 'test_mse': 1.3500580404644915}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.10693269679756985, 'test_mse': 0.9090353770737757}\n",
      "0.8998643335827607\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9235454863528484, 'test_mse': 1.2637617578407367}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.09901836188390829, 'test_mse': 1.0342639281356276}\n",
      "1.0315275476256445\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2949343622853586, 'test_mse': 1.5211530475638975}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.10192427992584076, 'test_mse': 1.1487077488572865}\n",
      "1.1348773268876853\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5005609400141362, 'test_mse': 1.2693548072542113}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.15077668227564378, 'test_mse': 1.2105791590570734}\n",
      "1.2509350106941282\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.8505871976614737, 'test_mse': 0.8587237246751538}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.07376607824431768, 'test_mse': 0.8249643650688979}\n",
      "0.8597644946613786\n",
      "progress 83.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3132866484363641, 'test_mse': 1.451006643147548}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.1026648549455386, 'test_mse': 1.2592678983121584}\n",
      "1.2541893737938203\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.0552358205430243, 'test_mse': 1.0563642040816807}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.07291131026277103, 'test_mse': 0.7963455837428929}\n",
      "0.8093855663932278\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4040858847921394, 'test_mse': 1.1006499716276386}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.07745155477223681, 'test_mse': 0.9968639904473484}\n",
      "1.0368539461721453\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4028121583426179, 'test_mse': 1.516431836324546}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.11154823860406651, 'test_mse': 1.1363852471097835}\n",
      "1.1412243626434293\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.42446231924915, 'test_mse': 1.2921233712841524}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.10082471097931167, 'test_mse': 1.1905941171886456}\n",
      "1.1652360180783141\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2279818598696282, 'test_mse': 1.497484421574383}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.10678231279275874, 'test_mse': 1.240468497664326}\n",
      "1.238051042286771\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2487799590230018, 'test_mse': 1.3520638529115816}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.08883328180756606, 'test_mse': 1.2329884933258728}\n",
      "1.2809742775499666\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3785748353613525, 'test_mse': 1.2170212582615707}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.091702427934209, 'test_mse': 0.9775742297757268}\n",
      "0.9998148524858635\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3316267007474798, 'test_mse': 1.3670831910965406}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 50 for first batch is {'train_mse': 0.07157930879890433, 'test_mse': 1.0446344820535074}\n",
      "1.0703706088220002\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3950458867973663, 'test_mse': 1.897036165489257}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.07148116099076565, 'test_mse': 1.4855472190736014}\n",
      "1.4824727105715099\n",
      "progress 91.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2842066440854558, 'test_mse': 1.0725683794232983}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.07549238172558292, 'test_mse': 0.9086562035785037}\n",
      "0.9101025683024493\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5987139168188202, 'test_mse': 1.5674997670861286}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.08251729094803326, 'test_mse': 1.0811006310297993}\n",
      "1.078380938519854\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5215422905357137, 'test_mse': 1.6174350036704386}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.05225711025789953, 'test_mse': 1.244508474127921}\n",
      "1.2551827488786722\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3367867134302276, 'test_mse': 1.312799656540045}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.06662783182912793, 'test_mse': 1.1134385949538201}\n",
      "1.0979904236310307\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1612597709410162, 'test_mse': 1.317361044074016}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.06857856600588438, 'test_mse': 0.960459459246318}\n",
      "0.9657752873062725\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3514780224522958, 'test_mse': 1.466424142988927}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.06748074150074061, 'test_mse': 0.9770822669314146}\n",
      "0.9394511118037515\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4442248812665608, 'test_mse': 1.1761176114216134}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.07170268281203444, 'test_mse': 0.9432909120714639}\n",
      "1.000020062365435\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 0.9540585417539841, 'test_mse': 1.3914011774981692}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.05337695521833131, 'test_mse': 1.2458795953448272}\n",
      "1.2419691103493358\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.028780218899447, 'test_mse': 1.248475716567587}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.07138066142147931, 'test_mse': 1.1508953834080564}\n",
      "1.150116780737812\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.356091022501976, 'test_mse': 1.7694635131968333}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.06269291481501142, 'test_mse': 0.9718337795510404}\n",
      "0.9233175004139924\n"
     ]
    }
   ],
   "source": [
    "d = 300\n",
    "n_train = 100\n",
    "n_test = 100\n",
    "\n",
    "N_SPACE = np.arange(0, 600, 50)\n",
    "\n",
    "l = []\n",
    "\n",
    "for i, N in enumerate(N_SPACE):\n",
    "  print(f\"progress {100*i/len(N_SPACE):.2f}%\")\n",
    "  l0 = []\n",
    "  for _ in range(10):\n",
    "    x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "\n",
    "    nn = Network(N, Relu, Linear)\n",
    "    config = dict(\n",
    "        nb_iterations=100, \n",
    "        batch_size=100, \n",
    "        learning_rate1=0.00, \n",
    "        learning_rate2=0.01\n",
    "    )\n",
    "    history = nn.fit(x_train, y_train, x_test, y_test, MSE, **config)\n",
    "    test_prediction = nn.predict(x_test)\n",
    "    final_test_error = nn.loss_function.loss(y_test, test_prediction)\n",
    "    l0.append(final_test_error)\n",
    "    print(final_test_error)\n",
    "  l0 = np.array(l0)\n",
    "  l.append({\"mean\": l0.mean(), \"std\": l0.std()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(l)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(-0.1, 2)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.053538</td>\n",
       "      <td>0.166176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.205967</td>\n",
       "      <td>0.156461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.202387</td>\n",
       "      <td>0.199249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.196825</td>\n",
       "      <td>0.142602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.167138</td>\n",
       "      <td>0.095058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.140646</td>\n",
       "      <td>0.214745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.178100</td>\n",
       "      <td>0.147432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.224021</td>\n",
       "      <td>0.118230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.109573</td>\n",
       "      <td>0.132817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.050130</td>\n",
       "      <td>0.129707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.147857</td>\n",
       "      <td>0.174436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.056231</td>\n",
       "      <td>0.122118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean       std\n",
       "0   1.053538  0.166176\n",
       "1   1.205967  0.156461\n",
       "2   1.202387  0.199249\n",
       "3   1.196825  0.142602\n",
       "4   1.167138  0.095058\n",
       "5   1.140646  0.214745\n",
       "6   1.178100  0.147432\n",
       "7   1.224021  0.118230\n",
       "8   1.109573  0.132817\n",
       "9   1.050130  0.129707\n",
       "10  1.147857  0.174436\n",
       "11  1.056231  0.122118"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train_mse': 1.356091022501976, 'test_mse': 1.7694635131968333},\n",
       " {'train_mse': 1.2017890877793052, 'test_mse': 1.3442425971786411},\n",
       " {'train_mse': 1.0738499783416875, 'test_mse': 1.5669696000525695},\n",
       " {'train_mse': 0.9662900679887837, 'test_mse': 1.2832309091898637},\n",
       " {'train_mse': 0.8747109710444577, 'test_mse': 1.435385762111958},\n",
       " {'train_mse': 0.7958476583682813, 'test_mse': 1.2429304041682685},\n",
       " {'train_mse': 0.7272498073118777, 'test_mse': 1.345072605593181},\n",
       " {'train_mse': 0.6670564661990571, 'test_mse': 1.2125356179904323},\n",
       " {'train_mse': 0.6138361714165977, 'test_mse': 1.2798807974364534},\n",
       " {'train_mse': 0.5664730552344565, 'test_mse': 1.18726046834139},\n",
       " {'train_mse': 0.5240853370815636, 'test_mse': 1.2307133008876985},\n",
       " {'train_mse': 0.4859666736677973, 'test_mse': 1.1650285159638982},\n",
       " {'train_mse': 0.4515436921355118, 'test_mse': 1.192249059311383},\n",
       " {'train_mse': 0.4203450197193924, 'test_mse': 1.144949172275593},\n",
       " {'train_mse': 0.3919785137238452, 'test_mse': 1.1612475567550096},\n",
       " {'train_mse': 0.36611436834508626, 'test_mse': 1.1266295682286884},\n",
       " {'train_mse': 0.3424724562432547, 'test_mse': 1.1356529331966332},\n",
       " {'train_mse': 0.3208127407595972, 'test_mse': 1.1098727117205707},\n",
       " {'train_mse': 0.30092793055914213, 'test_mse': 1.1141084648469772},\n",
       " {'train_mse': 0.28263778498292996, 'test_mse': 1.094551050349793},\n",
       " {'train_mse': 0.26578464531511686, 'test_mse': 1.09568559278828},\n",
       " {'train_mse': 0.2502298853080044, 'test_mse': 1.0805575514617491},\n",
       " {'train_mse': 0.23585105819023083, 'test_mse': 1.0797276882491882},\n",
       " {'train_mse': 0.22253957716693085, 'test_mse': 1.0677895835218545},\n",
       " {'train_mse': 0.21019880921420483, 'test_mse': 1.065756832826281},\n",
       " {'train_mse': 0.19874249275506273, 'test_mse': 1.0561456616327627},\n",
       " {'train_mse': 0.1880934120758601, 'test_mse': 1.053416260787257},\n",
       " {'train_mse': 0.17818227755863053, 'test_mse': 1.0455265045654243},\n",
       " {'train_mse': 0.16894677269661987, 'test_mse': 1.0424336425406968},\n",
       " {'train_mse': 0.1603307376499666, 'test_mse': 1.0358369709832398},\n",
       " {'train_mse': 0.152283465651094, 'test_mse': 1.032596952516418},\n",
       " {'train_mse': 0.14475909349952104, 'test_mse': 1.0269876695542322},\n",
       " {'train_mse': 0.13771607113147807, 'test_mse': 1.0237381800488914},\n",
       " {'train_mse': 0.13111669812445073, 'test_mse': 1.018895952950669},\n",
       " {'train_mse': 0.12492671722604202, 'test_mse': 1.0157220735783283},\n",
       " {'train_mse': 0.1191149567433603, 'test_mse': 1.011486343261933},\n",
       " {'train_mse': 0.11365301501207334, 'test_mse': 1.0084382008062407},\n",
       " {'train_mse': 0.10851498127014947, 'test_mse': 1.0046905312015435},\n",
       " {'train_mse': 0.10367718815415083, 'test_mse': 1.001795244096774},\n",
       " {'train_mse': 0.09911799176337377, 'test_mse': 0.998447092936827},\n",
       " {'train_mse': 0.09481757583481189, 'test_mse': 0.9957168329481728},\n",
       " {'train_mse': 0.09075777706686082, 'test_mse': 0.9927010402903946},\n",
       " {'train_mse': 0.08692192904247519, 'test_mse': 0.9901384519883759},\n",
       " {'train_mse': 0.08329472254902467, 'test_mse': 0.9874032879354862},\n",
       " {'train_mse': 0.07986208038471299, 'test_mse': 0.9850051132721611},\n",
       " {'train_mse': 0.07661104498983345, 'test_mse': 0.9825100938124707},\n",
       " {'train_mse': 0.073529677453021, 'test_mse': 0.9802695794214944},\n",
       " {'train_mse': 0.07060696662417063, 'test_mse': 0.9779825082545383},\n",
       " {'train_mse': 0.06783274722178753, 'test_mse': 0.975890989029763},\n",
       " {'train_mse': 0.06519762595724049, 'test_mse': 0.9737858526312774},\n",
       " {'train_mse': 0.06269291481501142, 'test_mse': 0.9718337795510404},\n",
       " {'train_mse': 0.060310570729288845, 'test_mse': 0.9698892384088116},\n",
       " {'train_mse': 0.05804314098540076, 'test_mse': 0.9680668328999034},\n",
       " {'train_mse': 0.05588371375150527, 'test_mse': 0.9662651311022329},\n",
       " {'train_mse': 0.05382587321323862, 'test_mse': 0.9645627898008423},\n",
       " {'train_mse': 0.05186365884298746, 'test_mse': 0.9628889596086985},\n",
       " {'train_mse': 0.04999152838723907, 'test_mse': 0.9612974935251338},\n",
       " {'train_mse': 0.048204324201022, 'test_mse': 0.9597387690678615},\n",
       " {'train_mse': 0.046497242598600746, 'test_mse': 0.9582495339965262},\n",
       " {'train_mse': 0.044865805925032365, 'test_mse': 0.9567949141352837},\n",
       " {'train_mse': 0.04330583708452557, 'test_mse': 0.9553998706453467},\n",
       " {'train_mse': 0.04181343628928714, 'test_mse': 0.9540397889832838},\n",
       " {'train_mse': 0.04038495981713556, 'test_mse': 0.9527315177310117},\n",
       " {'train_mse': 0.03901700058799971, 'test_mse': 0.9514575901981627},\n",
       " {'train_mse': 0.0377063703888298, 'test_mse': 0.9502292797429976},\n",
       " {'train_mse': 0.036450083593723336, 'test_mse': 0.9490341088482442},\n",
       " {'train_mse': 0.03524534224146136, 'test_mse': 0.947879527350014},\n",
       " {'train_mse': 0.034089522346380324, 'test_mse': 0.9467565482403123},\n",
       " {'train_mse': 0.032980161330766825, 'test_mse': 0.9456700064889842},\n",
       " {'train_mse': 0.031914946477922, 'test_mse': 0.9446133641910004},\n",
       " {'train_mse': 0.03089170431485089, 'test_mse': 0.9435896747748593},\n",
       " {'train_mse': 0.029908390842316047, 'test_mse': 0.9425941249702268},\n",
       " {'train_mse': 0.028963082537871374, 'test_mse': 0.941628560614388},\n",
       " {'train_mse': 0.028053968064558062, 'test_mse': 0.940689388399649},\n",
       " {'train_mse': 0.027179340624294016, 'test_mse': 0.9397776413251081},\n",
       " {'train_mse': 0.026337590900692938, 'test_mse': 0.9388905938957584},\n",
       " {'train_mse': 0.02552720054118325, 'test_mse': 0.93802873726893},\n",
       " {'train_mse': 0.024746736132920444, 'test_mse': 0.9371899675274891},\n",
       " {'train_mse': 0.023994843631150894, 'test_mse': 0.9363744195612074},\n",
       " {'train_mse': 0.023270243202443246, 'test_mse': 0.9355804384093263},\n",
       " {'train_mse': 0.02257172444859371, 'test_mse': 0.9348079293498727},\n",
       " {'train_mse': 0.021898141980074146, 'test_mse': 0.93405556497298},\n",
       " {'train_mse': 0.021248411310660895, 'test_mse': 0.9333231070034208},\n",
       " {'train_mse': 0.02062150504738587, 'test_mse': 0.932609469855218},\n",
       " {'train_mse': 0.020016449352218774, 'test_mse': 0.9319143298222023},\n",
       " {'train_mse': 0.019432320653941987, 'test_mse': 0.9312367823087547},\n",
       " {'train_mse': 0.018868242590542234, 'test_mse': 0.9305764571101584},\n",
       " {'train_mse': 0.018323383164130135, 'test_mse': 0.929932587189795},\n",
       " {'train_mse': 0.017796952091932407, 'test_mse': 0.9293047816254405},\n",
       " {'train_mse': 0.017288198338293384, 'test_mse': 0.9286923797025518},\n",
       " {'train_mse': 0.01679640781388723, 'test_mse': 0.9280949865771194},\n",
       " {'train_mse': 0.016320901229493926, 'test_mse': 0.9275120251903126},\n",
       " {'train_mse': 0.015861032092738713, 'test_mse': 0.9269431074580914},\n",
       " {'train_mse': 0.015416184837148907, 'test_mse': 0.9263877233567519},\n",
       " {'train_mse': 0.014985773073751174, 'test_mse': 0.9258454981064774},\n",
       " {'train_mse': 0.014569237956224272, 'test_mse': 0.9253159763822252},\n",
       " {'train_mse': 0.014166046651346626, 'test_mse': 0.9247988004733121},\n",
       " {'train_mse': 0.01377569090713709, 'test_mse': 0.9242935604695576},\n",
       " {'train_mse': 0.013397685711690916, 'test_mse': 0.9237999176462002},\n",
       " {'train_mse': 0.01303156803626462, 'test_mse': 0.9233175004139924}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
