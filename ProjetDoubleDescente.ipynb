{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.06430779e-03  8.45520389e-03 -1.74553159e-02  1.15756322e-02\n",
      " -5.93191140e-05 -6.29693077e-04  3.75684422e-03  7.51588416e-03\n",
      " -1.45883372e-02 -1.02691269e-02]\n",
      "[3.16227766 3.16227766 3.16227766 ... 3.16227766 3.16227766 3.16227766]\n"
     ]
    }
   ],
   "source": [
    "d = 10\n",
    "n = 10000\n",
    "\n",
    "sample_x = np.random.randn(n, d)\n",
    "mean = np.empty(d)\n",
    "norm_tab = np.empty(n)\n",
    "\n",
    "for i in range (0, n):\n",
    "    norm = np.sum(sample_x[i] * sample_x[i]) ** 0.5\n",
    "    sample_x[i] = (sample_x[i] / norm) * (d ** 0.5) \n",
    "\n",
    "#On vérifie que notre data set soit uniforme\n",
    "for i in range (0, d):\n",
    "    mean[i] = np.sum(sample_x.T[i]) / n\n",
    "\n",
    "#On vérifie que notre dataset ait une norme de sqrt(d)\n",
    "for i in range (0, n):\n",
    "    norm_tab[i] = np.sum(sample_x[i] * sample_x[i]) ** 0.5\n",
    "    \n",
    "print(mean)\n",
    "print(norm_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01650386]\n",
      " [-0.03180343]\n",
      " [ 0.09307829]\n",
      " ...\n",
      " [ 0.04341371]\n",
      " [-0.10715328]\n",
      " [ 0.038148  ]]\n"
     ]
    }
   ],
   "source": [
    "tau = 0.3 \n",
    "noise_level = tau * tau\n",
    "\n",
    "#On veut E(sample_noise) == 0 et E(sample_noise^2) == tau^2\n",
    "sample_noise = np.random.randn(n, 1) * noise_level\n",
    "\n",
    "print(sample_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "F1 = 1\n",
    "\n",
    "sample_params = np.random.randn(d, 1)\n",
    "    \n",
    "norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "#print(sample_params)\n",
    "print(np.sum(sample_params * sample_params) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55123855]\n",
      " [-2.11611324]\n",
      " [ 0.70684673]\n",
      " ...\n",
      " [ 0.70204034]\n",
      " [ 0.33374882]\n",
      " [ 1.12669184]]\n"
     ]
    }
   ],
   "source": [
    "sample_y = sample_x @ sample_params + sample_noise\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        s = Sigmoid.function(x)\n",
    "        return s * (1- s)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return np.maximum(0, x) / x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return (1/(y_real.shape[0] * y_real.shape[1])) * np.sum(np.sum((y_hat - y_real)**2, axis = 0))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/(y_real.shape[0] * y_real.shape[1])) * (y_hat - y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "        self.loss = None\n",
    "        \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(self.nb_layers -1):\n",
    "            self.weights[i + 1] = np.random.randn(self.dimensions[i], self.dimensions[i + 1]) / np.sqrt(self.dimensions[i])\n",
    "            self.bias[i + 1] = np.zeros(self.dimensions[i + 1])\n",
    "            \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \"\"\"\n",
    "        z = {}\n",
    "        a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        \n",
    "        for i in range(1, self.nb_layers):\n",
    "            z[i + 1] = a[i] @ self.weights[i] + self.bias[i] #Z = XW + b\n",
    "            a[i + 1] = self.activations[i + 1].function(z[i + 1]);\n",
    "            \n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, real_value):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(real_value, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = a[self.nb_layers - 1].T @ delta\n",
    "        \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        for i in reversed(range(2, self.nb_layers)):\n",
    "            delta = (delta @ self.weights[i].T) * self.activations[i].gradient(z[i])\n",
    "            partial_deriv = a[i - 1].T @ delta \n",
    "            update_parameters[i - 1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "        \n",
    "    def fit(self, x, y_real, loss, nb_iterations, batch_size, learning_rate1, learning_rate2):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            if(i % 10) == 0:\n",
    "                z, a = self.forward_pass(x)\n",
    "                print(\"Loss at Iteration {} is {}\".format(i, self.loss_function.loss(y_real, a[self.nb_layers])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 0 is 0.9715030721531901\n",
      "Loss at Iteration 10 is 0.7369101213738476\n",
      "Loss at Iteration 20 is 0.7382480305588923\n",
      "Loss at Iteration 30 is 0.7377807545456785\n",
      "Loss at Iteration 40 is 0.736460788727149\n",
      "Loss at Iteration 50 is 0.7345780734992224\n",
      "Loss at Iteration 60 is 0.7320788066231054\n",
      "Loss at Iteration 70 is 0.7287405895577195\n",
      "Loss at Iteration 80 is 0.7251039438835585\n",
      "Loss at Iteration 90 is 0.7211542096693944\n"
     ]
    }
   ],
   "source": [
    "#Test if our architecture Compiles\n",
    "nn = Network(10000, Relu, Sigmoid)\n",
    "nn.fit(sample_x, sample_y, MSE, 100, 10000, 0.03, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
