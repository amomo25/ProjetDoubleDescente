{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55966322  1.91968871 -0.17764174 ...  0.27136752  0.10383974\n",
      "  -1.13842675]\n",
      " [-1.53562404 -1.28052297  1.30303947 ...  0.70378453 -0.3563811\n",
      "  -1.5376784 ]\n",
      " [ 1.03679244 -2.83290421 -1.6156227  ... -0.66153441  1.3240896\n",
      "   0.99324659]\n",
      " ...\n",
      " [-1.05228301  0.6449208   0.881983   ... -0.13614209  0.32997485\n",
      "  -0.41205993]\n",
      " [ 0.38517788  0.67230177  1.10773559 ...  0.10345264 -0.63418472\n",
      "   0.99945811]\n",
      " [-1.01190348 -0.30861521 -0.60055545 ... -0.41960398 -0.42631601\n",
      "  -1.42870796]]\n",
      "[-0.13460775  0.02073274 -0.05123846 -0.03684429  0.06999158  0.02428018\n",
      " -0.01698616 -0.16808936 -0.12925297  0.03011144  0.03206035  0.22422625\n",
      " -0.02876989  0.08304679 -0.09237388  0.04244089 -0.08768415 -0.02061218\n",
      "  0.12593252 -0.04465747 -0.20865592 -0.18546071 -0.08045186  0.0318762\n",
      "  0.0672875   0.08386133  0.14002296  0.07206153  0.12305841 -0.00396065\n",
      "  0.12790397 -0.04115929 -0.00978955  0.00663603  0.11648248 -0.09221591\n",
      "  0.11825496  0.04870093  0.187799    0.10517194  0.16929694  0.01496919\n",
      " -0.19852634  0.02587963  0.08998616  0.06174415 -0.00394909 -0.1277956\n",
      " -0.20284613 -0.20341282  0.05888464  0.09418109  0.09321671 -0.04499764\n",
      " -0.00280918 -0.0475553   0.01141911 -0.00497296  0.07702723 -0.07258685\n",
      "  0.05332615 -0.0128495   0.07985593 -0.14026707 -0.11283381 -0.11008783\n",
      "  0.0068341   0.02308446  0.04941035 -0.10742544  0.04065471  0.06130271\n",
      "  0.03563439 -0.12054637 -0.08768831 -0.00475379 -0.04660768 -0.08202087\n",
      " -0.00242988  0.01849992 -0.02604514  0.03776601 -0.02863036  0.07027374\n",
      " -0.17285507  0.15773158  0.27256769 -0.05955675  0.06104883  0.02225823\n",
      " -0.00970727  0.04111987 -0.09619422 -0.09995422  0.08801932  0.0377823\n",
      " -0.06578283  0.19662347 -0.06666416 -0.06993548]\n",
      "[10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "d = 100\n",
    "n_train = 100\n",
    "n_test = 100\n",
    "\n",
    "x_train = np.random.randn(n_train, d)\n",
    "x_test = np.random.randn(n_test, d)\n",
    "mean_train = np.empty(d)\n",
    "norm_tab = np.empty(n_train)\n",
    "\n",
    "norm_train = (np.sum(x_train * x_train, axis = 1) ** 0.5).reshape(n_train, 1)\n",
    "x_train = (x_train / norm_train) * (d ** 0.5)\n",
    "    \n",
    "norm_test = (np.sum(x_test * x_test, axis = 1) **0.5).reshape(n_test, 1)\n",
    "x_test = (x_test / norm_test) * (d**0.5)\n",
    "\n",
    "#On vérifie que notre data set soit uniforme (sur chaque dimension, le dataset a à peu près une moyenne de 0)\n",
    "#On se rapproche de plus en plus de 0 lorsqu'on augmente n_train, les données étant mieux réparties\n",
    "mean_train = np.sum(x_train, axis = 0) / n_train\n",
    "    \n",
    "#On vérifie que chaque élément de notre dataset ait une norme de sqrt(d)\n",
    "norm_tab = np.sum(x_train * x_train, axis = 1) ** 0.5\n",
    "    \n",
    "print(x_train)\n",
    "print(mean_train)\n",
    "print(norm_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12781377]\n",
      " [-0.03923029]\n",
      " [-0.07904442]\n",
      " [ 0.05741477]\n",
      " [-0.04845852]\n",
      " [ 0.01453979]\n",
      " [ 0.02755737]\n",
      " [-0.00175861]\n",
      " [ 0.07077686]\n",
      " [-0.07868615]\n",
      " [-0.04787398]\n",
      " [ 0.06709918]\n",
      " [-0.11015352]\n",
      " [ 0.1750214 ]\n",
      " [ 0.06575702]\n",
      " [-0.03954415]\n",
      " [-0.17609548]\n",
      " [ 0.07310113]\n",
      " [-0.06981979]\n",
      " [ 0.02879374]\n",
      " [-0.03039039]\n",
      " [-0.04331537]\n",
      " [ 0.00406155]\n",
      " [ 0.06411468]\n",
      " [-0.06498312]\n",
      " [-0.10635058]\n",
      " [ 0.02802123]\n",
      " [ 0.12020815]\n",
      " [ 0.09089286]\n",
      " [ 0.10570188]\n",
      " [-0.1241159 ]\n",
      " [ 0.04403384]\n",
      " [ 0.11180185]\n",
      " [ 0.10712743]\n",
      " [-0.00072577]\n",
      " [ 0.03498522]\n",
      " [-0.06701235]\n",
      " [ 0.04489236]\n",
      " [ 0.14139007]\n",
      " [-0.05461176]\n",
      " [-0.16959004]\n",
      " [-0.02545919]\n",
      " [ 0.13778267]\n",
      " [ 0.07950076]\n",
      " [-0.06812214]\n",
      " [ 0.03387911]\n",
      " [-0.04050964]\n",
      " [-0.03501495]\n",
      " [-0.04630265]\n",
      " [-0.14722693]\n",
      " [ 0.00259115]\n",
      " [-0.05154168]\n",
      " [-0.11665923]\n",
      " [ 0.12348315]\n",
      " [ 0.00347504]\n",
      " [-0.15107072]\n",
      " [ 0.09628691]\n",
      " [ 0.09206547]\n",
      " [-0.03401757]\n",
      " [-0.05117602]\n",
      " [ 0.06627562]\n",
      " [ 0.09326562]\n",
      " [ 0.16250485]\n",
      " [-0.03211107]\n",
      " [ 0.07912254]\n",
      " [ 0.04832567]\n",
      " [ 0.16078578]\n",
      " [ 0.08521263]\n",
      " [-0.1339559 ]\n",
      " [ 0.0597152 ]\n",
      " [ 0.05938227]\n",
      " [ 0.03277132]\n",
      " [-0.1676356 ]\n",
      " [ 0.09095451]\n",
      " [ 0.1002893 ]\n",
      " [ 0.04234439]\n",
      " [ 0.01750209]\n",
      " [-0.01391361]\n",
      " [ 0.09850641]\n",
      " [ 0.14214739]\n",
      " [ 0.09298686]\n",
      " [ 0.03687469]\n",
      " [ 0.09023162]\n",
      " [-0.01441039]\n",
      " [-0.07522228]\n",
      " [-0.00397356]\n",
      " [-0.15715776]\n",
      " [ 0.08711973]\n",
      " [-0.06335833]\n",
      " [-0.02742558]\n",
      " [-0.04610822]\n",
      " [ 0.20843959]\n",
      " [-0.14183365]\n",
      " [-0.03604359]\n",
      " [ 0.04453336]\n",
      " [-0.02211435]\n",
      " [ 0.02066327]\n",
      " [ 0.02072631]\n",
      " [ 0.16633026]\n",
      " [-0.02187985]]\n"
     ]
    }
   ],
   "source": [
    "tau = 0.3 \n",
    "noise_level = tau * tau\n",
    "\n",
    "#On veut E(noise) == 0 et E(noise^2) == tau^2\n",
    "noise_train = np.random.randn(n_train, 1) * noise_level\n",
    "noise_test = np.random.randn(n_test, 1) * noise_level\n",
    "\n",
    "print(noise_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08636917]\n",
      " [ 0.15059657]\n",
      " [-0.05041674]\n",
      " [ 0.02407247]\n",
      " [ 0.16721414]\n",
      " [ 0.08299572]\n",
      " [-0.14551764]\n",
      " [-0.12729818]\n",
      " [ 0.06843926]\n",
      " [-0.0448261 ]\n",
      " [-0.01633027]\n",
      " [ 0.10661415]\n",
      " [-0.12850346]\n",
      " [-0.16461718]\n",
      " [-0.14357349]\n",
      " [-0.0023188 ]\n",
      " [-0.15722676]\n",
      " [-0.08021234]\n",
      " [-0.04449533]\n",
      " [-0.07836209]\n",
      " [-0.1296414 ]\n",
      " [-0.03909144]\n",
      " [-0.02956377]\n",
      " [ 0.09196313]\n",
      " [ 0.08101829]\n",
      " [ 0.05375191]\n",
      " [-0.23507774]\n",
      " [ 0.13673358]\n",
      " [ 0.20639965]\n",
      " [ 0.11016033]\n",
      " [ 0.1252628 ]\n",
      " [-0.02816475]\n",
      " [-0.08193507]\n",
      " [-0.04297887]\n",
      " [-0.21678094]\n",
      " [ 0.04527932]\n",
      " [ 0.14240804]\n",
      " [-0.06522507]\n",
      " [ 0.07086588]\n",
      " [ 0.00645926]\n",
      " [-0.07099331]\n",
      " [ 0.13305495]\n",
      " [ 0.05992755]\n",
      " [-0.03894004]\n",
      " [-0.00073344]\n",
      " [ 0.07506835]\n",
      " [-0.06197657]\n",
      " [ 0.06728207]\n",
      " [-0.13543681]\n",
      " [ 0.03446349]\n",
      " [ 0.03527505]\n",
      " [ 0.01531769]\n",
      " [-0.0845779 ]\n",
      " [ 0.02764105]\n",
      " [ 0.05890864]\n",
      " [ 0.00118302]\n",
      " [ 0.05036781]\n",
      " [ 0.0336304 ]\n",
      " [ 0.06723106]\n",
      " [-0.0089023 ]\n",
      " [ 0.0592506 ]\n",
      " [-0.15458409]\n",
      " [ 0.02042158]\n",
      " [-0.02028615]\n",
      " [ 0.17068723]\n",
      " [ 0.02401497]\n",
      " [-0.2098327 ]\n",
      " [ 0.13742144]\n",
      " [-0.00580048]\n",
      " [-0.03405038]\n",
      " [-0.05046997]\n",
      " [ 0.12416609]\n",
      " [ 0.16351541]\n",
      " [ 0.10009917]\n",
      " [-0.07726472]\n",
      " [ 0.03060802]\n",
      " [ 0.12570337]\n",
      " [ 0.12933723]\n",
      " [-0.02791464]\n",
      " [ 0.21732654]\n",
      " [-0.04568477]\n",
      " [-0.09099121]\n",
      " [ 0.02232766]\n",
      " [ 0.12785824]\n",
      " [-0.01913743]\n",
      " [-0.05406013]\n",
      " [ 0.05833058]\n",
      " [ 0.18459751]\n",
      " [-0.04787653]\n",
      " [ 0.15020279]\n",
      " [-0.20494644]\n",
      " [-0.01319201]\n",
      " [ 0.07769314]\n",
      " [ 0.02107282]\n",
      " [-0.04746633]\n",
      " [ 0.07320137]\n",
      " [ 0.05685061]\n",
      " [ 0.00906124]\n",
      " [ 0.05797865]\n",
      " [ 0.02565569]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "F1 = 1\n",
    "\n",
    "sample_params = np.random.randn(d, 1)\n",
    "    \n",
    "norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "print(sample_params)\n",
    "print(np.sum(sample_params * sample_params) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.31685713e+00]\n",
      " [ 1.61921551e+00]\n",
      " [-5.15564036e-01]\n",
      " [-7.86201766e-01]\n",
      " [-1.02264398e+00]\n",
      " [ 1.30527258e+00]\n",
      " [ 2.08673681e+00]\n",
      " [ 8.62651309e-01]\n",
      " [-8.00677185e-01]\n",
      " [-2.44123420e+00]\n",
      " [ 3.24313016e-01]\n",
      " [ 6.32396318e-01]\n",
      " [-1.27890868e+00]\n",
      " [-5.03502764e-01]\n",
      " [-8.16028202e-01]\n",
      " [-2.20471352e-01]\n",
      " [-1.10596696e-01]\n",
      " [ 1.00326067e+00]\n",
      " [-8.60210341e-02]\n",
      " [-5.11143033e-01]\n",
      " [ 8.06675411e-01]\n",
      " [ 1.94312574e+00]\n",
      " [ 2.03468569e+00]\n",
      " [-1.19107381e-01]\n",
      " [-1.26682466e+00]\n",
      " [-1.46961890e+00]\n",
      " [-4.06844054e-02]\n",
      " [-4.25729510e-01]\n",
      " [-6.21786764e-01]\n",
      " [-5.74683686e-01]\n",
      " [-6.61137467e-01]\n",
      " [ 1.41654547e+00]\n",
      " [ 3.97176465e-01]\n",
      " [-6.55314797e-01]\n",
      " [ 5.06083921e-01]\n",
      " [-1.50739650e+00]\n",
      " [-1.05832990e+00]\n",
      " [ 9.86420696e-01]\n",
      " [ 3.89657110e-01]\n",
      " [-1.69619043e-01]\n",
      " [ 2.39282986e-01]\n",
      " [ 1.53857057e+00]\n",
      " [ 5.61334710e-01]\n",
      " [ 1.48354139e+00]\n",
      " [ 6.85544726e-01]\n",
      " [-1.90777223e+00]\n",
      " [-1.29789706e+00]\n",
      " [ 7.61534928e-01]\n",
      " [-3.34122914e-01]\n",
      " [-5.94031014e-01]\n",
      " [-9.64121681e-01]\n",
      " [-1.08745849e+00]\n",
      " [ 3.12624995e-01]\n",
      " [ 1.33503818e+00]\n",
      " [-1.61010887e-01]\n",
      " [ 8.06239222e-01]\n",
      " [-3.37289037e-01]\n",
      " [ 7.30226293e-01]\n",
      " [-2.22727525e-01]\n",
      " [-1.69454231e-01]\n",
      " [ 3.08195863e-01]\n",
      " [ 2.83400584e-01]\n",
      " [ 4.05758816e-01]\n",
      " [ 3.16489605e-01]\n",
      " [ 2.40998124e-03]\n",
      " [ 1.74499631e+00]\n",
      " [ 8.40024132e-01]\n",
      " [-1.34205283e+00]\n",
      " [ 5.68988233e-01]\n",
      " [ 2.16964895e+00]\n",
      " [-4.35015307e-01]\n",
      " [ 7.42850772e-02]\n",
      " [-1.19314057e-01]\n",
      " [-6.21835946e-01]\n",
      " [-3.81244160e-01]\n",
      " [ 6.88511771e-02]\n",
      " [ 3.08572080e-02]\n",
      " [ 8.01576548e-02]\n",
      " [-4.57096761e-02]\n",
      " [ 6.65657018e-01]\n",
      " [-5.16418265e-02]\n",
      " [ 9.44172489e-01]\n",
      " [-1.17607529e+00]\n",
      " [ 5.49551310e-01]\n",
      " [-7.80819983e-01]\n",
      " [ 1.41451401e+00]\n",
      " [ 7.26630355e-01]\n",
      " [ 1.29822925e+00]\n",
      " [ 7.91255862e-02]\n",
      " [ 2.30496750e+00]\n",
      " [ 3.21066924e-01]\n",
      " [ 7.41256455e-01]\n",
      " [ 7.96712975e-01]\n",
      " [ 9.88156819e-01]\n",
      " [-8.39223249e-01]\n",
      " [-1.34402618e+00]\n",
      " [ 1.50240493e-01]\n",
      " [ 1.30369633e-01]\n",
      " [-7.64900037e-02]\n",
      " [ 8.85239811e-01]]\n"
     ]
    }
   ],
   "source": [
    "y_train = x_train @ sample_params + noise_train\n",
    "y_test = x_test @ sample_params + noise_test\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        s = Sigmoid.function(x)\n",
    "        return s * (1- s)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.tanh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh.function(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x * (x >= 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return x >= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return (1/(y_real.shape[0] * y_real.shape[1])) * np.sum(np.sum((y_hat - y_real)**2, axis = 0))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/(y_real.shape[0] * y_real.shape[1])) * (y_hat - y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "        \n",
    "        self.loss_tab = None\n",
    "        \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(self.nb_layers -1):\n",
    "            self.weights[i + 1] = np.random.randn(self.dimensions[i], self.dimensions[i + 1]) / np.sqrt(self.dimensions[i])\n",
    "            self.bias[i + 1] = np.zeros(self.dimensions[i + 1])\n",
    "            \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \"\"\"\n",
    "        z = {}\n",
    "        a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        \n",
    "        for i in range(1, self.nb_layers):\n",
    "            z[i + 1] = a[i] @ self.weights[i] + self.bias[i] #Z = XW + b\n",
    "            a[i + 1] = self.activations[i + 1].function(z[i + 1]);\n",
    "            \n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, real_value):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(real_value, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = a[self.nb_layers - 1].T @ delta\n",
    "        \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        for i in reversed(range(2, self.nb_layers)):\n",
    "            delta = (delta @ self.weights[i].T) * self.activations[i].gradient(z[i])\n",
    "            partial_deriv = a[i - 1].T @ delta \n",
    "            update_parameters[i - 1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "        \n",
    "    def fit(self, x, y_real, loss, nb_iterations, batch_size, learning_rate1, learning_rate2):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "            \n",
    "        self.loss_tab = np.empty((nb_iterations, x.shape[0] // batch_size))\n",
    "        \n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.loss_tab[i][j] = self.loss_function.loss(y_real[start:end], a[self.nb_layers])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            if(i % 10) == 0:\n",
    "                print(\"Loss at Iteration {} for first batch is {}\".format(i, self.loss_tab[i][0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 0 for first batch is 2.133102808588878\n",
      "Loss at Iteration 10 for first batch is 1.8701541651372027\n",
      "Loss at Iteration 20 for first batch is 1.811810446895198\n",
      "Loss at Iteration 30 for first batch is 1.7652767287492814\n",
      "Loss at Iteration 40 for first batch is 1.7277852817870247\n",
      "Loss at Iteration 50 for first batch is 1.6971355772525698\n",
      "Loss at Iteration 60 for first batch is 1.6716851361399307\n",
      "Loss at Iteration 70 for first batch is 1.6502364156784886\n",
      "Loss at Iteration 80 for first batch is 1.6319192195408083\n",
      "Loss at Iteration 90 for first batch is 1.6160955707780447\n",
      "Loss at Iteration 100 for first batch is 1.6022901542375014\n",
      "1.0706659940297758\n"
     ]
    }
   ],
   "source": [
    "nn = Network(200, Sigmoid, Sigmoid)\n",
    "nn.fit(x_train, y_train, MSE, 101, 10, 0, 0.03)\n",
    "\n",
    "z, a = nn.forward_pass(x_test)\n",
    "print(nn.loss_function.loss(y_test, a[nn.nb_layers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
