{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(d, n_train, n_test):\n",
    "  x_train = np.random.randn(n_train, d)\n",
    "  x_test = np.random.randn(n_test, d)\n",
    "  mean_train = np.empty(d)\n",
    "  norm_tab = np.empty(n_train)\n",
    "\n",
    "  norm_train = (np.sum(x_train * x_train, axis = 1) ** 0.5).reshape(n_train, 1)\n",
    "  x_train = (x_train / norm_train) * (d ** 0.5)\n",
    "      \n",
    "  norm_test = (np.sum(x_test * x_test, axis = 1) **0.5).reshape(n_test, 1)\n",
    "  x_test = (x_test / norm_test) * (d**0.5)\n",
    "\n",
    "  #On vérifie que notre data set soit uniforme (sur chaque dimension, le dataset a à peu près une moyenne de 0)\n",
    "  #On se rapproche de plus en plus de 0 lorsqu'on augmente n_train, les données étant mieux réparties\n",
    "  mean_train = np.sum(x_train, axis = 0) / n_train\n",
    "      \n",
    "  #On vérifie que chaque élément de notre dataset ait une norme de sqrt(d)\n",
    "  norm_tab = np.sum(x_train * x_train, axis = 1) ** 0.5\n",
    "      \n",
    "  tau = 0.\n",
    "  noise_level = tau * tau\n",
    "  #On veut E(noise) == 0 et E(noise^2) == tau^2\n",
    "  noise_train = np.random.randn(n_train, 1) * noise_level\n",
    "  noise_test = np.random.randn(n_test, 1) * noise_level\n",
    "\n",
    "  F1 = 1\n",
    "\n",
    "  sample_params = np.random.randn(d, 1)\n",
    "      \n",
    "  norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "  sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "  y_train = np.dot(x_train, sample_params) + noise_train\n",
    "  y_test = np.dot(x_test, sample_params) + noise_test\n",
    "\n",
    "  return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return Sigmoid.function(x) * Sigmoid.function(-x)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.tanh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh.function(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return x >= 0\n",
    "\n",
    "class Linear :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return np.ones((x.shape[0], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return np.mean(np.sum((y_hat.T - y_real.T)**2, axis = 0))    \n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/y_real.shape[0]) * (y_hat - y_real)\n",
    "    \n",
    "class reg_MSE:\n",
    "    def __init__(self, weights, lam):\n",
    "        self.weights = weights\n",
    "        self.lam = lam\n",
    "        \n",
    "    def loss(self, y_real, y_hat):\n",
    "        loss = np.mean(np.sum((y_hat.T - y_real.T)**2, axis = 0))\n",
    "        reg = (np.sum(np.square(self.weights[1])) + np.sum(np.square(self.weights[2])))*(self.lam * self.weights[1].shape[1]/self.weights[1].shape[0])\n",
    "        return loss + reg\n",
    "        \n",
    "    def gradient(self, y_real, y_hat):\n",
    "        loss_grad = (2/y_real.shape[0]) * (y_hat - y_real)\n",
    "        reg_grad = (self.lam * self.weights[1].shape[1]/self.weights[1].shape[0]) * (np.sum(self.weights[1]) + np.sum(self.weights[2]))\n",
    "        return loss_grad + reg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "                \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "                \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(1, self.nb_layers):\n",
    "            self.weights[i] = np.random.randn(self.dimensions[i - 1], self.dimensions[i]) / np.sqrt(self.dimensions[i - 1])\n",
    "            self.bias[i] = np.zeros(self.dimensions[i])\n",
    "         \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \n",
    "        return : z contient les paramètres avant que l'on applique l'activation function\n",
    "        return : a contient les paramètres après que l'on applique l'activation function\n",
    "        \"\"\"\n",
    "        z = {}\n",
    "        a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        for i in range(1, self.nb_layers):\n",
    "            z[i + 1] = np.dot(a[i], self.weights[i]) + self.bias[i] #Z = XW + b\n",
    "            a[i + 1] = self.activations[i + 1].function(z[i + 1])\n",
    "            \n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, y_real):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(y_real, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = np.dot(a[self.nb_layers - 1].T, delta)\n",
    "     \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'hidden layer\n",
    "        delta = np.dot(delta, self.weights[2].T) * self.activations[2].gradient(z[2])\n",
    "        partial_deriv = np.dot(a[1].T, delta) \n",
    "        update_parameters[1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_real, x_test, y_test, loss, nb_iterations = 100, batch_size = 100, learning_rate1 = 0, learning_rate2 = 0.3):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "            \n",
    "        loss_tab = []\n",
    "        \n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            _, a_train = self.forward_pass(x)\n",
    "            _, a_test = self.forward_pass(x_test)\n",
    "            loss_tab.append({\n",
    "                \"train_mse\":  self.loss_function.loss(y_real, a_train[self.nb_layers]),\n",
    "                \"test_mse\":  self.loss_function.loss(y_test, a_test[self.nb_layers]),\n",
    "            })\n",
    "            #if(i % 50) == 0:\n",
    "                #print(f\"Loss at Iteration {i} for first batch is {loss_tab[-1]}\")\n",
    "        return loss_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = 100\n",
    "n_train = 300\n",
    "n_test = 300\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "nn = Network(200, Linear, Linear)\n",
    "config = dict(\n",
    "        nb_iterations=30000, \n",
    "        batch_size=300, \n",
    "        learning_rate1=0, \n",
    "        learning_rate2=0.001\n",
    "    )\n",
    "\n",
    "loss_func = MSE #reg_MSE(nn.weights, 10**(-8))\n",
    "history = nn.fit(x_train, y_train, x_test, y_test, loss_func, **config)\n",
    "\n",
    "test_prediction = nn.predict(x_test)\n",
    "print(test_prediction)\n",
    "\n",
    "#print(y_test[0])\n",
    "\n",
    "beta_calc = np.dot(np.dot(np.linalg.inv(np.dot(x_train.T, x_train)), x_train.T), y_train)\n",
    "real_test_prediction = np.dot(beta_calc.T, x_test.T)\n",
    "print(\"#########################\")\n",
    "print(real_test_prediction)\n",
    "print(MSE.loss(test_prediction, real_test_prediction.T))\n",
    "\n",
    "df = pd.DataFrame(history)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(-0.1, 3)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the formula\n",
    "\n",
    "d = 1000\n",
    "n_test = 300\n",
    "\n",
    "gammas = np.arange(0.1, 3, 0.1)\n",
    "\n",
    "l_f = []\n",
    "\n",
    "for i, gam in enumerate(gammas):\n",
    "    n_train = int((1/gam) * d)\n",
    "    print(f\"progress {100*i/len(gammas):.2f}%, n = {n_train}, gamma = {gam}\")\n",
    "    l0_f = []\n",
    "    for _ in range(5):\n",
    "        x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "        beta_calc = np.dot(np.dot(np.linalg.inv(np.dot(x_train.T, x_train)), x_train.T), y_train)\n",
    "        test_prediction = np.dot(beta_calc.T, x_test.T)\n",
    "        test_error = MSE.loss(y_test, test_prediction.T)\n",
    "        l0_f.append(test_error)\n",
    "        print(test_error)\n",
    "    l0_f = np.array(l0_f)\n",
    "    l_f.append({\"mean\": l0_f.mean(), \"std\": l0_f.std()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(l_f)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(-0.1, 2)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the neural network\n",
    "\n",
    "d = 50\n",
    "n_train = 150\n",
    "n_test = 50\n",
    "\n",
    "N_SPACE = np.arange(0.1, 2, 0.1)\n",
    "\n",
    "l = []\n",
    "\n",
    "for i, N in enumerate(N_SPACE):\n",
    "    N = int(N*n_train)\n",
    "    print(f\"progress {100*i/len(N_SPACE):.2f}%, N = {N}\")\n",
    "    l0 = []\n",
    "    for _ in range(10):\n",
    "        x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "\n",
    "        nn = Network(N, Relu, Linear)\n",
    "        config = dict(\n",
    "            nb_iterations=500, \n",
    "            batch_size=n_train, \n",
    "            learning_rate1=0., \n",
    "            learning_rate2=0.01\n",
    "        )\n",
    "        loss_func = reg_MSE(nn.weights, 10**(-5))\n",
    "        history = nn.fit(x_train, y_train, x_test, y_test, loss_func, **config)\n",
    "        test_prediction = nn.predict(x_test)\n",
    "        final_test_error = nn.loss_function.loss(y_test, test_prediction)\n",
    "        l0.append(final_test_error)\n",
    "        print(final_test_error)\n",
    "    l0 = np.array(l0)\n",
    "    l.append({\"mean\": l0.mean(), \"std\": l0.std()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(l)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(-0.1, 2)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
