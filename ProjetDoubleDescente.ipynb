{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73040249  1.02638497 -0.81810604 ... -0.82806821  0.7046309\n",
      "  -0.73831226]\n",
      " [ 0.53352603  0.8678703   0.46480233 ...  0.20677645  0.75264725\n",
      "   0.32644224]\n",
      " [ 1.41769954 -0.81525512  2.22452376 ... -0.68956001  0.67944808\n",
      "  -0.02809299]\n",
      " ...\n",
      " [ 2.33988197 -0.24042937  0.62444028 ...  0.062087   -1.03178924\n",
      "  -0.93674059]\n",
      " [ 0.96338506  0.90169186  0.61309972 ...  0.06837548 -2.1173958\n",
      "  -0.84982692]\n",
      " [ 0.77424809 -1.54355043  1.02010147 ... -0.46824251  0.60118562\n",
      "  -1.00639204]]\n",
      "[ 2.20761041e-02  5.28713482e-03  1.84415591e-03  3.02767790e-03\n",
      " -4.39955179e-03  1.10287317e-02 -2.74008996e-03 -6.94401296e-04\n",
      " -4.57161200e-03 -6.77995641e-04  8.58845259e-03 -3.54065786e-03\n",
      " -4.80209039e-04 -4.42399424e-03 -9.56730662e-03  7.66975150e-03\n",
      "  9.12429909e-03 -6.21580253e-04  3.05275854e-03 -2.42610100e-03\n",
      "  1.54018489e-02  1.82812180e-03  5.35761485e-03  7.24776577e-03\n",
      "  4.39833178e-03 -1.65122260e-03  1.34959863e-02  1.98479754e-02\n",
      " -8.82987035e-03  1.18999388e-02 -1.58493332e-03 -1.23143189e-02\n",
      "  5.52086458e-03  2.47398608e-02  7.82937834e-03  1.15821275e-02\n",
      " -1.77773286e-02  1.04247026e-02 -9.63373777e-03 -2.43436747e-02\n",
      " -9.35267248e-03 -1.35983080e-02 -1.04467167e-02  1.04628438e-02\n",
      " -4.17151103e-03 -2.27170624e-02  5.80223282e-03  4.66127765e-03\n",
      " -3.81764642e-03  1.32778334e-02  4.77310018e-03 -2.00888298e-02\n",
      " -8.36046263e-03 -8.12814721e-03 -7.49685425e-03 -6.37406343e-03\n",
      "  3.26537217e-05  1.39438508e-03  5.13743632e-03  3.65464682e-03\n",
      " -8.13951826e-03 -1.34629675e-02 -1.07257629e-02  2.93643863e-02\n",
      "  9.92301316e-03  5.25368179e-03 -7.48868435e-03 -7.19752899e-03\n",
      " -3.93992448e-03 -2.05638998e-02  1.01423930e-03  9.51960374e-03\n",
      " -7.36584506e-03  4.04422340e-03 -4.22653855e-03 -9.11754224e-03\n",
      " -1.25348335e-02 -1.10374738e-02  1.47207289e-02  1.09804638e-02\n",
      "  4.36625984e-03 -1.02881105e-05  1.30163430e-02  7.47390385e-05\n",
      "  1.02559345e-02 -2.61748871e-03  1.10638355e-02 -1.22642655e-02\n",
      " -3.72499167e-03 -8.20827444e-03 -5.85366020e-03 -5.60123976e-03\n",
      "  1.82223319e-02 -1.53449127e-02  1.10591076e-02 -3.49797662e-04\n",
      " -1.13837244e-02  1.45946590e-02 -5.02303912e-03  1.72569618e-03]\n",
      "[10. 10. 10. ... 10. 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "d = 100\n",
    "n_train = 10000\n",
    "n_test = 100\n",
    "\n",
    "x_train = np.random.randn(n_train, d)\n",
    "x_test = np.random.randn(n_test, d)\n",
    "mean_train = np.empty(d)\n",
    "norm_tab = np.empty(n_train)\n",
    "\n",
    "norm_train = (np.sum(x_train * x_train, axis = 1) ** 0.5).reshape(n_train, 1)\n",
    "x_train = (x_train / norm_train) * (d ** 0.5)\n",
    "    \n",
    "norm_test = (np.sum(x_test * x_test, axis = 1) **0.5).reshape(n_test, 1)\n",
    "x_test = (x_test / norm_test) * (d**0.5)\n",
    "\n",
    "#On vérifie que notre data set soit uniforme (sur chaque dimension, le dataset a à peu près une moyenne de 0)\n",
    "#On se rapproche de plus en plus de 0 lorsqu'on augmente n_train, les données étant mieux réparties\n",
    "mean_train = np.sum(x_train, axis = 0) / n_train\n",
    "    \n",
    "#On vérifie que chaque élément de notre dataset ait une norme de sqrt(d)\n",
    "norm_tab = np.sum(x_train * x_train, axis = 1) ** 0.5\n",
    "    \n",
    "print(x_train)\n",
    "print(mean_train)\n",
    "print(norm_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06220055]\n",
      " [ 0.02542597]\n",
      " [-0.08308046]\n",
      " ...\n",
      " [-0.00403558]\n",
      " [ 0.05062063]\n",
      " [-0.09573005]]\n"
     ]
    }
   ],
   "source": [
    "tau = 0.3 \n",
    "noise_level = tau * tau\n",
    "\n",
    "#On veut E(noise) == 0 et E(noise^2) == tau^2\n",
    "noise_train = np.random.randn(n_train, 1) * noise_level\n",
    "noise_test = np.random.randn(n_test, 1) * noise_level\n",
    "\n",
    "print(noise_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38370807e-01]\n",
      " [-7.23731504e-02]\n",
      " [ 1.95168973e-01]\n",
      " [-9.21434089e-03]\n",
      " [ 1.21440897e-02]\n",
      " [-4.03075693e-02]\n",
      " [ 1.00238459e-01]\n",
      " [-1.50791317e-02]\n",
      " [ 5.80444834e-02]\n",
      " [-4.27294901e-02]\n",
      " [-1.16539979e-02]\n",
      " [ 1.36454570e-01]\n",
      " [ 1.39873445e-02]\n",
      " [ 1.76773940e-01]\n",
      " [ 3.94239364e-02]\n",
      " [-1.16733625e-01]\n",
      " [ 5.43733942e-02]\n",
      " [ 1.07172340e-01]\n",
      " [-5.19719204e-02]\n",
      " [-2.40138220e-01]\n",
      " [-1.88738171e-02]\n",
      " [ 1.87183576e-02]\n",
      " [-9.15205171e-02]\n",
      " [ 9.85751443e-02]\n",
      " [ 9.74773532e-04]\n",
      " [ 1.10482395e-01]\n",
      " [-2.18108538e-03]\n",
      " [-1.45315772e-01]\n",
      " [ 6.16550931e-02]\n",
      " [-3.14770477e-02]\n",
      " [ 2.87169345e-02]\n",
      " [-1.37941414e-01]\n",
      " [-1.26560530e-01]\n",
      " [ 1.25949405e-02]\n",
      " [ 2.17530070e-01]\n",
      " [-1.39768287e-01]\n",
      " [-5.49404138e-03]\n",
      " [-2.13431175e-01]\n",
      " [ 5.53432351e-03]\n",
      " [ 3.97663120e-03]\n",
      " [ 5.73169766e-02]\n",
      " [ 8.21412000e-02]\n",
      " [-1.08499844e-01]\n",
      " [-2.04469636e-01]\n",
      " [-3.04108571e-02]\n",
      " [-5.32722962e-02]\n",
      " [-5.17307492e-02]\n",
      " [-1.01404710e-01]\n",
      " [-3.00817975e-01]\n",
      " [-1.33790632e-01]\n",
      " [-1.31114713e-01]\n",
      " [-1.21097907e-01]\n",
      " [-1.30760978e-01]\n",
      " [ 3.86646862e-02]\n",
      " [ 1.88763431e-01]\n",
      " [ 3.76955603e-02]\n",
      " [-1.81254624e-01]\n",
      " [ 1.08636969e-01]\n",
      " [-7.26018530e-03]\n",
      " [-7.22111539e-02]\n",
      " [ 1.11469681e-01]\n",
      " [ 8.36236381e-02]\n",
      " [-2.05794201e-01]\n",
      " [ 3.20006774e-02]\n",
      " [ 8.32209604e-02]\n",
      " [-4.02665951e-03]\n",
      " [ 1.10563128e-01]\n",
      " [-3.24197621e-02]\n",
      " [-7.61476947e-02]\n",
      " [ 1.01125209e-02]\n",
      " [ 2.26224358e-04]\n",
      " [-7.06941008e-02]\n",
      " [-1.39230872e-02]\n",
      " [-5.74876056e-02]\n",
      " [-9.38972639e-02]\n",
      " [ 2.58213550e-02]\n",
      " [-9.98339288e-02]\n",
      " [ 1.11564223e-03]\n",
      " [-1.02187881e-01]\n",
      " [-1.04264865e-01]\n",
      " [ 1.19616164e-01]\n",
      " [-3.81701163e-02]\n",
      " [-7.77587048e-02]\n",
      " [-3.52056300e-02]\n",
      " [ 3.48880894e-02]\n",
      " [-4.92778534e-02]\n",
      " [ 1.32372897e-02]\n",
      " [-1.43410850e-01]\n",
      " [ 2.41841910e-04]\n",
      " [ 9.86097160e-03]\n",
      " [-1.64063215e-01]\n",
      " [-2.75394672e-02]\n",
      " [ 8.92719802e-03]\n",
      " [-4.88239688e-02]\n",
      " [ 1.29620650e-01]\n",
      " [-9.80969644e-02]\n",
      " [ 6.52242346e-02]\n",
      " [ 1.51049728e-02]\n",
      " [ 7.03890573e-03]\n",
      " [ 4.02466165e-02]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "F1 = 1\n",
    "\n",
    "sample_params = np.random.randn(d, 1)\n",
    "    \n",
    "norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "print(sample_params)\n",
    "print(np.sum(sample_params * sample_params) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.93785434]\n",
      " [-0.1226976 ]\n",
      " [-1.12436936]\n",
      " ...\n",
      " [ 2.5616995 ]\n",
      " [ 0.84768215]\n",
      " [ 0.40611687]]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.dot(x_train, sample_params) + noise_train\n",
    "y_test = np.dot(x_test, sample_params) + noise_test\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        s = Sigmoid.function(x)\n",
    "        return s * (1- s)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.tanh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh.function(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x * (x >= 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return x >= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return (1/(y_real.shape[0])) * np.sum((y_hat.T - y_real.T)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/(y_real.shape[0])) * (y_hat - y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "                \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(self.nb_layers -1):\n",
    "            self.weights[i + 1] = np.random.randn(self.dimensions[i], self.dimensions[i + 1]) / np.sqrt(self.dimensions[i])\n",
    "            self.bias[i + 1] = np.zeros(self.dimensions[i + 1])\n",
    "         \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \n",
    "        return : z contient les paramètres avant que l'on applique l'activation function\n",
    "        return : a contient les paramètres après que l'on applique l'activation function\n",
    "        \"\"\"\n",
    "        z = {}\n",
    "        a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        for i in range(1, self.nb_layers):\n",
    "            z[i + 1] = np.dot(a[i], self.weights[i]) + self.bias[i] #Z = XW + b\n",
    "            a[i + 1] = self.activations[i + 1].function(z[i + 1])\n",
    "            \n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, y_real):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(y_real, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = np.dot(a[self.nb_layers - 1].T, delta)\n",
    "        \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        for i in reversed(range(2, self.nb_layers)):\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activations[i].gradient(z[i])\n",
    "            partial_deriv = np.dot(a[i - 1].T, delta) \n",
    "            update_parameters[i - 1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_real, x_test, y_test, loss, nb_iterations = 100, batch_size = 100, learning_rate1 = 0, learning_rate2 = 0.3):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "            \n",
    "        loss_tab = []\n",
    "        \n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            _, a_train = self.forward_pass(x)\n",
    "            _, a_test = self.forward_pass(x_test)\n",
    "            loss_tab.append({\n",
    "                \"train_mse\":  self.loss_function.loss(y_real, a_train[self.nb_layers]),\n",
    "                \"test_mse\":  self.loss_function.loss(y_test, a_test[self.nb_layers]),\n",
    "            })\n",
    "            if(i % 10) == 0:\n",
    "                print(f\"Loss at Iteration {i} for first batch is {loss_tab[-1]}\")\n",
    "        return loss_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 0 for first batch is {'train_mse': 1.012110336665785, 'test_mse': 1.0690902580482726}\n",
      "Loss at Iteration 10 for first batch is {'train_mse': 0.9992446754654821, 'test_mse': 1.0204562746060477}\n",
      "Loss at Iteration 20 for first batch is {'train_mse': 0.9988211381392916, 'test_mse': 1.0178289274064773}\n",
      "Loss at Iteration 30 for first batch is {'train_mse': 0.9986281419993757, 'test_mse': 1.017289703781498}\n",
      "Loss at Iteration 40 for first batch is {'train_mse': 0.9957998798660871, 'test_mse': 1.016227637154182}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.9813359956373888, 'test_mse': 1.0092604779811658}\n",
      "Loss at Iteration 60 for first batch is {'train_mse': 0.9452267830401568, 'test_mse': 0.9821351653465137}\n",
      "Loss at Iteration 70 for first batch is {'train_mse': 0.8988423941823042, 'test_mse': 0.9434305304004559}\n",
      "Loss at Iteration 80 for first batch is {'train_mse': 0.8541913808085992, 'test_mse': 0.8973828117759625}\n",
      "Loss at Iteration 90 for first batch is {'train_mse': 0.8134928571828474, 'test_mse': 0.8537980873504983}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.7780293939548558, 'test_mse': 0.8307054672924202}\n",
      "Loss at Iteration 110 for first batch is {'train_mse': 0.7467491113756213, 'test_mse': 0.8242179472716983}\n",
      "Loss at Iteration 120 for first batch is {'train_mse': 0.7211386481050149, 'test_mse': 0.8161655892156252}\n",
      "Loss at Iteration 130 for first batch is {'train_mse': 0.7002696483121864, 'test_mse': 0.7593913115214123}\n",
      "Loss at Iteration 140 for first batch is {'train_mse': 0.6813984296898474, 'test_mse': 0.7397878188666054}\n",
      "Loss at Iteration 150 for first batch is {'train_mse': 0.6679603565766694, 'test_mse': 0.7396832230345458}\n",
      "Loss at Iteration 160 for first batch is {'train_mse': 0.6566522720494701, 'test_mse': 0.7395964859764107}\n",
      "Loss at Iteration 170 for first batch is {'train_mse': 0.6458762221984182, 'test_mse': 0.7334133553708319}\n",
      "Loss at Iteration 180 for first batch is {'train_mse': 0.6387840627562923, 'test_mse': 0.7337744239458444}\n",
      "Loss at Iteration 190 for first batch is {'train_mse': 0.6332612527037561, 'test_mse': 0.7251245868315661}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.6295137742758562, 'test_mse': 0.7081426689972176}\n",
      "Loss at Iteration 210 for first batch is {'train_mse': 0.6266820671689509, 'test_mse': 0.7104678254089606}\n",
      "Loss at Iteration 220 for first batch is {'train_mse': 0.6239194350575421, 'test_mse': 0.7038489917876227}\n",
      "Loss at Iteration 230 for first batch is {'train_mse': 0.6212826270177226, 'test_mse': 0.7039019024066993}\n",
      "Loss at Iteration 240 for first batch is {'train_mse': 0.6190954184219234, 'test_mse': 0.7039029650149492}\n",
      "Loss at Iteration 250 for first batch is {'train_mse': 0.6168158249542107, 'test_mse': 0.7039029640514695}\n",
      "Loss at Iteration 260 for first batch is {'train_mse': 0.6157122791882731, 'test_mse': 0.7039025721697172}\n",
      "Loss at Iteration 270 for first batch is {'train_mse': 0.6151094376310878, 'test_mse': 0.7036531140298948}\n",
      "Loss at Iteration 280 for first batch is {'train_mse': 0.6140139298509211, 'test_mse': 0.7006708364897658}\n",
      "Loss at Iteration 290 for first batch is {'train_mse': 0.6136346311102847, 'test_mse': 0.7053515675283138}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.6129860152900634, 'test_mse': 0.7054057436679491}\n",
      "Loss at Iteration 310 for first batch is {'train_mse': 0.6128473755013141, 'test_mse': 0.7054058470328893}\n",
      "Loss at Iteration 320 for first batch is {'train_mse': 0.61244969236519, 'test_mse': 0.7054057903052818}\n",
      "Loss at Iteration 330 for first batch is {'train_mse': 0.6122131481913337, 'test_mse': 0.7053976468907314}\n",
      "Loss at Iteration 340 for first batch is {'train_mse': 0.6117733085892632, 'test_mse': 0.7040451935456912}\n",
      "Loss at Iteration 350 for first batch is {'train_mse': 0.6116212353430698, 'test_mse': 0.7016578004390022}\n",
      "Loss at Iteration 360 for first batch is {'train_mse': 0.6113911077459264, 'test_mse': 0.7018784778537372}\n",
      "Loss at Iteration 370 for first batch is {'train_mse': 0.6114587913784002, 'test_mse': 0.7018789071953323}\n",
      "Loss at Iteration 380 for first batch is {'train_mse': 0.6114242062635132, 'test_mse': 0.7018321277217383}\n",
      "Loss at Iteration 390 for first batch is {'train_mse': 0.6115186853765164, 'test_mse': 0.6984500883496682}\n",
      "[[1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.59358206e-274]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.12927264e-049]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [7.22196232e-266]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [7.73727928e-001]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [5.51727881e-127]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [6.22537377e-273]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [9.82464901e-001]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.70163452e-255]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [9.96640089e-207]\n",
      " [5.19649189e-089]\n",
      " [7.69253689e-072]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [6.68479323e-001]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [4.37255909e-059]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.00000000e+000]]\n",
      "0.6997076961746531\n",
      "[ 1.5374548  -0.15358487 -0.56477453 -1.09500662 -0.04568145 -0.63340814\n",
      "  0.70258409 -0.97883688 -0.50348825  0.48045762  0.45751444  1.89065891\n",
      "  0.90041989  0.0761991   0.07387566 -0.21324634  0.19616004 -0.38234941\n",
      "  0.69427493 -0.02137532 -1.54101165  0.31186889  1.39574696  0.97513018\n",
      "  0.29684626  1.71676647 -1.11710509 -1.76703921 -1.21222216 -1.27693703\n",
      " -1.61292348  1.14912688  1.54537468 -0.98231724  0.29125212 -0.72107457\n",
      " -0.90701965 -0.50161924  0.74097232 -0.13307743  0.05268232 -0.40254131\n",
      " -1.88464832 -0.01221152  0.55180283 -0.7555803  -1.30567026 -1.05569167\n",
      "  1.72877943  0.48128614 -0.57798456 -0.21470627 -0.44407508 -1.58319151\n",
      " -1.52608955 -0.05774376 -1.17390362  0.00897809  0.14237398  0.57909085\n",
      " -0.51570913 -1.61994619 -0.66804305 -0.18326504 -0.42547197  0.17989942\n",
      "  0.35625328  0.44242929 -1.40614418  0.95349321  0.76728006  0.21432215\n",
      "  0.0385718  -0.30449742 -0.10073347 -0.26259216  1.11002492  0.66305762\n",
      " -0.11539574 -1.12608438 -0.69212167  0.55474151 -1.05044412 -0.0632272\n",
      " -1.37614818  0.0150354  -0.65642915 -1.20931786 -0.22725969  0.89713572\n",
      " -0.02670152  1.45638128 -0.20373534  0.07478495  1.11953016  0.54329539\n",
      " -0.75897677  1.07975518 -1.63216862  0.83706865]\n"
     ]
    }
   ],
   "source": [
    "nn = Network(200, Sigmoid, Sigmoid)\n",
    "config = dict(\n",
    "    nb_iterations=400, \n",
    "    batch_size=1000, \n",
    "    learning_rate1=0.03, \n",
    "    learning_rate2=0.03\n",
    ")\n",
    "history = nn.fit(x_train, y_train, x_test, y_test, MSE, **config)\n",
    "test_prediction = nn.predict(x_test)\n",
    "print(test_prediction)\n",
    "print(nn.loss_function.loss(y_test, test_prediction))\n",
    "\n",
    "clf = MLPRegressor(hidden_layer_sizes = (200), activation = 'logistic', batch_size = 100, learning_rate_init = 0.03, max_iter = 400, shuffle = False)\n",
    "clf.fit(x_train, np.reshape(y_train, -1))\n",
    "\n",
    "test_prediction = clf.predict(x_test)\n",
    "print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(history)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(0.3, 1.25)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
