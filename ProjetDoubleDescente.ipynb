{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.24394385  0.35888723  0.61335998 ...  0.98851465 -2.09171879\n",
      "   0.20681687]\n",
      " [-0.5886257  -0.91065224 -0.64233817 ... -0.15324536 -2.75650141\n",
      "   1.81628187]\n",
      " [-0.00791393  0.41906952  1.54886544 ...  1.27082023 -0.03408474\n",
      "   0.08149221]\n",
      " ...\n",
      " [ 0.00503757  0.61333048  0.13076353 ...  0.99811433  1.40087712\n",
      "   1.4480464 ]\n",
      " [-0.46866276  0.82421084 -1.21539935 ... -0.67031569 -0.39785709\n",
      "   0.84030733]\n",
      " [ 0.83294493 -1.98413371 -0.64235233 ... -0.07186019 -0.70303711\n",
      "  -0.55130839]]\n",
      "[-0.01110746 -0.01323612 -0.0162666  -0.00040109  0.01704995 -0.00462313\n",
      "  0.00371022 -0.00275605 -0.01560954  0.00776071  0.01305669  0.02185381\n",
      "  0.01293718 -0.01244828 -0.00023919  0.01808282  0.00622444 -0.00246574\n",
      " -0.01048145  0.01510435 -0.00462324  0.00047801  0.00716352  0.01411778\n",
      " -0.01286868 -0.00025532 -0.01013294  0.01177309  0.00631262 -0.0050639\n",
      "  0.00322641 -0.01668625 -0.00573494 -0.01298262  0.00881646  0.01223162\n",
      " -0.00646538  0.00448398 -0.01935319 -0.0022587  -0.01438639 -0.0262222\n",
      " -0.00613607  0.00246267  0.01318958 -0.00363138  0.01345913 -0.01071307\n",
      " -0.0082523  -0.01223869 -0.00621972 -0.01771901 -0.00098011 -0.0026868\n",
      "  0.02090712  0.00455635 -0.00134812  0.01439254 -0.0131725  -0.02091393\n",
      "  0.00780945  0.00110653  0.00285439 -0.00572006  0.00506619 -0.00080392\n",
      "  0.00478363  0.0222127  -0.01347792 -0.00363552 -0.0080848   0.00785298\n",
      "  0.00700739 -0.02363119  0.00089364  0.01262893 -0.00943697  0.01074414\n",
      "  0.0034776  -0.00674031 -0.01035819 -0.01308061 -0.00138344  0.00159563\n",
      " -0.00463137  0.01546688 -0.00489495  0.00982389 -0.00427762  0.01895733\n",
      " -0.00831236  0.00379307  0.01571959 -0.0056599   0.00047291  0.00212061\n",
      "  0.0089906  -0.01793844 -0.00645782  0.01166967]\n",
      "[10. 10. 10. ... 10. 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "d = 100\n",
    "n_train = 10000\n",
    "n_test = 100\n",
    "\n",
    "x_train = np.random.randn(n_train, d)\n",
    "x_test = np.random.randn(n_test, d)\n",
    "mean_train = np.empty(d)\n",
    "norm_tab = np.empty(n_train)\n",
    "\n",
    "norm_train = (np.sum(x_train * x_train, axis = 1) ** 0.5).reshape(n_train, 1)\n",
    "x_train = (x_train / norm_train) * (d ** 0.5)\n",
    "    \n",
    "norm_test = (np.sum(x_test * x_test, axis = 1) **0.5).reshape(n_test, 1)\n",
    "x_test = (x_test / norm_test) * (d**0.5)\n",
    "\n",
    "#On vérifie que notre data set soit uniforme (sur chaque dimension, le dataset a à peu près une moyenne de 0)\n",
    "#On se rapproche de plus en plus de 0 lorsqu'on augmente n_train, les données étant mieux réparties\n",
    "mean_train = np.sum(x_train, axis = 0) / n_train\n",
    "    \n",
    "#On vérifie que chaque élément de notre dataset ait une norme de sqrt(d)\n",
    "norm_tab = np.sum(x_train * x_train, axis = 1) ** 0.5\n",
    "    \n",
    "print(x_train)\n",
    "print(mean_train)\n",
    "print(norm_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05966267]\n",
      " [-0.08706804]\n",
      " [-0.0631737 ]\n",
      " ...\n",
      " [ 0.06438079]\n",
      " [ 0.01911872]\n",
      " [ 0.02393865]]\n"
     ]
    }
   ],
   "source": [
    "tau = 0.3 \n",
    "noise_level = tau * tau\n",
    "\n",
    "#On veut E(noise) == 0 et E(noise^2) == tau^2\n",
    "noise_train = np.random.randn(n_train, 1) * noise_level\n",
    "noise_test = np.random.randn(n_test, 1) * noise_level\n",
    "\n",
    "print(noise_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.06757559e-01]\n",
      " [-3.44893573e-04]\n",
      " [-4.95584837e-02]\n",
      " [ 8.87751668e-02]\n",
      " [ 8.22857371e-02]\n",
      " [-2.02997082e-01]\n",
      " [ 2.12829682e-01]\n",
      " [ 5.42864581e-02]\n",
      " [ 2.81778946e-02]\n",
      " [ 2.29384462e-02]\n",
      " [-2.05354424e-01]\n",
      " [-3.81038707e-02]\n",
      " [ 7.74121262e-02]\n",
      " [ 4.21548797e-02]\n",
      " [ 3.91952034e-02]\n",
      " [ 4.23591103e-02]\n",
      " [ 6.04566838e-02]\n",
      " [ 2.88965045e-02]\n",
      " [-1.09546487e-02]\n",
      " [ 3.04100503e-02]\n",
      " [ 5.38438635e-02]\n",
      " [ 9.47429423e-02]\n",
      " [ 3.18237968e-02]\n",
      " [-9.26011396e-02]\n",
      " [-6.15946560e-03]\n",
      " [ 2.90096799e-02]\n",
      " [-5.88837344e-04]\n",
      " [-1.91553387e-02]\n",
      " [-1.67713864e-01]\n",
      " [-2.47939081e-03]\n",
      " [-1.72475528e-01]\n",
      " [ 3.13635988e-02]\n",
      " [-1.66387644e-01]\n",
      " [-8.35706327e-02]\n",
      " [ 7.41885080e-02]\n",
      " [-1.85801294e-01]\n",
      " [-5.66136210e-02]\n",
      " [ 5.36758770e-02]\n",
      " [-6.40646047e-02]\n",
      " [-8.11086651e-02]\n",
      " [ 1.57162613e-02]\n",
      " [-1.19852802e-01]\n",
      " [-6.98414582e-03]\n",
      " [ 1.36635292e-01]\n",
      " [ 1.19834644e-01]\n",
      " [-3.60046962e-01]\n",
      " [-1.50480560e-01]\n",
      " [-6.35888808e-02]\n",
      " [ 1.65374180e-02]\n",
      " [-1.22264529e-02]\n",
      " [ 1.23852283e-01]\n",
      " [-6.84744765e-02]\n",
      " [-6.98580341e-02]\n",
      " [ 1.17003240e-01]\n",
      " [-4.38093965e-02]\n",
      " [ 2.96541068e-02]\n",
      " [ 1.53749641e-02]\n",
      " [ 7.47181952e-02]\n",
      " [-1.30999572e-02]\n",
      " [ 8.61529618e-03]\n",
      " [-1.50548281e-01]\n",
      " [-8.24575907e-02]\n",
      " [ 1.09010308e-01]\n",
      " [ 2.28883925e-01]\n",
      " [-1.20479184e-02]\n",
      " [-2.43237338e-01]\n",
      " [ 4.67056942e-02]\n",
      " [ 2.15047870e-03]\n",
      " [ 1.42158508e-01]\n",
      " [-2.04084976e-02]\n",
      " [-4.07977064e-02]\n",
      " [ 9.46898269e-02]\n",
      " [-7.63456499e-03]\n",
      " [-6.17112680e-04]\n",
      " [ 1.44150289e-01]\n",
      " [-6.41326859e-02]\n",
      " [-4.56773323e-02]\n",
      " [-5.43325991e-02]\n",
      " [ 4.50343299e-02]\n",
      " [-7.21093488e-02]\n",
      " [ 5.26698234e-02]\n",
      " [-1.56112180e-01]\n",
      " [-8.14190653e-02]\n",
      " [ 4.69791937e-02]\n",
      " [-1.55167455e-01]\n",
      " [ 1.03134082e-02]\n",
      " [ 2.75451393e-02]\n",
      " [-4.28203889e-02]\n",
      " [-6.42416913e-02]\n",
      " [-7.58080100e-02]\n",
      " [-1.20815357e-01]\n",
      " [-3.75478838e-02]\n",
      " [ 1.38470870e-01]\n",
      " [-1.04647043e-04]\n",
      " [-3.01273758e-03]\n",
      " [-1.01904872e-01]\n",
      " [-1.06573204e-01]\n",
      " [-1.69810693e-01]\n",
      " [ 1.28807004e-02]\n",
      " [-1.15158622e-01]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "F1 = 1\n",
    "\n",
    "sample_params = np.random.randn(d, 1)\n",
    "    \n",
    "norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "print(sample_params)\n",
    "print(np.sum(sample_params * sample_params) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.46753555]\n",
      " [-1.00601337]\n",
      " [-0.32271178]\n",
      " ...\n",
      " [-0.43226771]\n",
      " [-0.01921792]\n",
      " [-2.09180129]]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.dot(x_train, sample_params) + noise_train\n",
    "y_test = np.dot(x_test, sample_params) + noise_test\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        s = Sigmoid.function(x)\n",
    "        return s * (1- s)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.tanh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh.function(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x * (x >= 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return x >= 0\n",
    "\n",
    "class Linear :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return np.ones((x.shape[0], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return np.mean(np.sum((y_hat.T - y_real.T)**2, axis = 0))    \n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/y_real.shape[0]) * (y_hat - y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "                \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(1, self.nb_layers):\n",
    "            self.weights[i] = np.random.randn(self.dimensions[i - 1], self.dimensions[i]) / np.sqrt(self.dimensions[i - 1])\n",
    "            self.bias[i] = np.zeros(self.dimensions[i])\n",
    "         \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \n",
    "        return : z contient les paramètres avant que l'on applique l'activation function\n",
    "        return : a contient les paramètres après que l'on applique l'activation function\n",
    "        \"\"\"\n",
    "        z = {}\n",
    "        a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        for i in range(1, self.nb_layers):\n",
    "            z[i + 1] = np.dot(a[i], self.weights[i]) + self.bias[i] #Z = XW + b\n",
    "            a[i + 1] = self.activations[i + 1].function(z[i + 1])\n",
    "            \n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, y_real):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(y_real, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = np.dot(a[self.nb_layers - 1].T, delta)\n",
    "     \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'hidden layer\n",
    "        delta = np.dot(delta, self.weights[2].T) * self.activations[2].gradient(z[2])\n",
    "        partial_deriv = np.dot(a[1].T, delta) \n",
    "        update_parameters[1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_real, x_test, y_test, loss, nb_iterations = 100, batch_size = 100, learning_rate1 = 0, learning_rate2 = 0.3):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "            \n",
    "        loss_tab = []\n",
    "        \n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            _, a_train = self.forward_pass(x)\n",
    "            _, a_test = self.forward_pass(x_test)\n",
    "            loss_tab.append({\n",
    "                \"train_mse\":  self.loss_function.loss(y_real, a_train[self.nb_layers]),\n",
    "                \"test_mse\":  self.loss_function.loss(y_test, a_test[self.nb_layers]),\n",
    "            })\n",
    "            if(i % 50) == 0:\n",
    "                print(f\"Loss at Iteration {i} for first batch is {loss_tab[-1]}\")\n",
    "        return loss_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 0 for first batch is {'train_mse': 0.6401923613164876, 'test_mse': 0.5462765241749327}\n",
      "Loss at Iteration 50 for first batch is {'train_mse': 0.011102825779700754, 'test_mse': 0.010936510532597928}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.008290774823098276, 'test_mse': 0.007838609203755339}\n",
      "Loss at Iteration 150 for first batch is {'train_mse': 0.007902237889681744, 'test_mse': 0.0073086762411561715}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.00782152576880147, 'test_mse': 0.007185073025332852}\n",
      "Loss at Iteration 250 for first batch is {'train_mse': 0.007797220860887626, 'test_mse': 0.007214933439202212}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.007786225899988952, 'test_mse': 0.007235091205962683}\n",
      "Loss at Iteration 350 for first batch is {'train_mse': 0.007776979787611459, 'test_mse': 0.007242276575537192}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.007767673176333521, 'test_mse': 0.0072556453450243075}\n",
      "Loss at Iteration 450 for first batch is {'train_mse': 0.007758328106163245, 'test_mse': 0.007279308377662985}\n",
      "Loss at Iteration 500 for first batch is {'train_mse': 0.007750214520532526, 'test_mse': 0.007261782099992623}\n",
      "Loss at Iteration 550 for first batch is {'train_mse': 0.00774264157575019, 'test_mse': 0.007280858609143589}\n",
      "Loss at Iteration 600 for first batch is {'train_mse': 0.007735584051447283, 'test_mse': 0.007309603916181804}\n",
      "Loss at Iteration 650 for first batch is {'train_mse': 0.0077288363719966585, 'test_mse': 0.007323834801668908}\n",
      "Loss at Iteration 700 for first batch is {'train_mse': 0.007721119436539295, 'test_mse': 0.0073422565027389175}\n",
      "Loss at Iteration 750 for first batch is {'train_mse': 0.007714492623026932, 'test_mse': 0.0073636831299220395}\n",
      "Loss at Iteration 800 for first batch is {'train_mse': 0.0077074700040201906, 'test_mse': 0.007363050844601187}\n",
      "Loss at Iteration 850 for first batch is {'train_mse': 0.007700103559054877, 'test_mse': 0.00736757874212464}\n",
      "Loss at Iteration 900 for first batch is {'train_mse': 0.007693062176269352, 'test_mse': 0.007386327617569332}\n",
      "Loss at Iteration 950 for first batch is {'train_mse': 0.007686181370750761, 'test_mse': 0.007398388370464786}\n",
      "[[-1.40001632]\n",
      " [ 0.22211705]\n",
      " [ 0.88379574]\n",
      " [ 0.86306222]\n",
      " [ 0.10118553]\n",
      " [-0.48614926]\n",
      " [ 0.1120232 ]\n",
      " [ 2.00017358]\n",
      " [ 0.28391863]\n",
      " [-0.3265656 ]\n",
      " [ 0.6454702 ]\n",
      " [-0.41722328]\n",
      " [ 0.38685406]\n",
      " [ 0.30294541]\n",
      " [ 0.15413731]\n",
      " [ 0.45816525]\n",
      " [-0.93004876]\n",
      " [ 0.38386655]\n",
      " [ 1.00158487]\n",
      " [-0.01340252]\n",
      " [-0.09820311]\n",
      " [ 0.3369647 ]\n",
      " [-0.57651592]\n",
      " [-1.24497114]\n",
      " [-0.31365573]\n",
      " [-0.66343161]\n",
      " [ 0.16427127]\n",
      " [-1.3091203 ]\n",
      " [ 0.1176662 ]\n",
      " [-0.12817459]\n",
      " [ 1.48334489]\n",
      " [ 0.69196159]\n",
      " [-0.69100796]\n",
      " [-0.96922895]\n",
      " [-1.23489957]\n",
      " [-1.77645325]\n",
      " [-0.07577058]\n",
      " [-0.51317999]\n",
      " [ 0.01885777]\n",
      " [-0.25655797]\n",
      " [-0.64182647]\n",
      " [-0.97966241]\n",
      " [ 0.91250786]\n",
      " [ 0.47206704]\n",
      " [ 0.32911789]\n",
      " [ 0.60817335]\n",
      " [-0.16261556]\n",
      " [ 0.84252887]\n",
      " [-0.7536538 ]\n",
      " [-0.29671314]\n",
      " [-0.49027668]\n",
      " [-0.83945046]\n",
      " [-1.44275107]\n",
      " [-0.3991701 ]\n",
      " [ 1.40121275]\n",
      " [-0.152096  ]\n",
      " [-0.84362034]\n",
      " [-0.21583673]\n",
      " [ 0.26849245]\n",
      " [ 1.11164274]\n",
      " [-1.96033695]\n",
      " [-0.4931997 ]\n",
      " [ 0.78535133]\n",
      " [ 0.36558448]\n",
      " [ 0.25221121]\n",
      " [-1.11487465]\n",
      " [-0.44417071]\n",
      " [ 0.73036192]\n",
      " [ 0.61589243]\n",
      " [ 1.81359213]\n",
      " [ 2.50877328]\n",
      " [ 0.24984512]\n",
      " [-0.19993464]\n",
      " [ 1.95683218]\n",
      " [-0.12065873]\n",
      " [-0.13431074]\n",
      " [-1.24780176]\n",
      " [ 0.09844137]\n",
      " [ 0.7177657 ]\n",
      " [ 0.26451715]\n",
      " [-0.60419897]\n",
      " [ 0.88124462]\n",
      " [-0.99862662]\n",
      " [-0.67661081]\n",
      " [ 0.68379649]\n",
      " [ 0.60736826]\n",
      " [-1.50911777]\n",
      " [-0.51094602]\n",
      " [ 0.98058602]\n",
      " [-0.57457422]\n",
      " [ 0.55896284]\n",
      " [ 0.22686074]\n",
      " [-0.14885269]\n",
      " [-0.81231618]\n",
      " [-1.10263963]\n",
      " [ 1.1434187 ]\n",
      " [-0.67774579]\n",
      " [ 0.65629615]\n",
      " [-1.25178437]\n",
      " [ 2.33284462]]\n",
      "0.0074096944437728575\n"
     ]
    }
   ],
   "source": [
    "nn = Network(20, Relu, Linear)\n",
    "config = dict(\n",
    "    nb_iterations=1000, \n",
    "    batch_size=100, \n",
    "    learning_rate1=0.01, \n",
    "    learning_rate2=0.01\n",
    ")\n",
    "history = nn.fit(x_train, y_train, x_test, y_test, MSE, **config)\n",
    "test_prediction = nn.predict(x_test)\n",
    "print(test_prediction)\n",
    "print(nn.loss_function.loss(y_test, test_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(history)\n",
    "df.plot(grid=True)\n",
    "plt.ylim(-0.1, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
