{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(d, n_train, n_test):\n",
    "  x_train = np.random.randn(n_train, d)\n",
    "  x_test = np.random.randn(n_test, d)\n",
    "  mean_train = np.empty(d)\n",
    "  norm_tab = np.empty(n_train)\n",
    "\n",
    "  norm_train = (np.sum(x_train * x_train, axis = 1) ** 0.5).reshape(n_train, 1)\n",
    "  x_train = (x_train / norm_train) * (d ** 0.5)\n",
    "      \n",
    "  norm_test = (np.sum(x_test * x_test, axis = 1) **0.5).reshape(n_test, 1)\n",
    "  x_test = (x_test / norm_test) * (d**0.5)\n",
    "\n",
    "  #On vérifie que notre data set soit uniforme (sur chaque dimension, le dataset a à peu près une moyenne de 0)\n",
    "  #On se rapproche de plus en plus de 0 lorsqu'on augmente n_train, les données étant mieux réparties\n",
    "  mean_train = np.sum(x_train, axis = 0) / n_train\n",
    "      \n",
    "  #On vérifie que chaque élément de notre dataset ait une norme de sqrt(d)\n",
    "  norm_tab = np.sum(x_train * x_train, axis = 1) ** 0.5\n",
    "      \n",
    "  tau = 0.\n",
    "  noise_level = tau * tau\n",
    "  #On veut E(noise) == 0 et E(noise^2) == tau^2\n",
    "  noise_train = np.random.randn(n_train, 1) * noise_level\n",
    "  noise_test = np.random.randn(n_test, 1) * noise_level\n",
    "\n",
    "  F1 = 1\n",
    "\n",
    "  sample_params = np.random.randn(d, 1)\n",
    "      \n",
    "  norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "  sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "  y_train = np.dot(x_train, sample_params) + noise_train\n",
    "  y_test = np.dot(x_test, sample_params) + noise_test\n",
    "\n",
    "  return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return Sigmoid.function(x) * Sigmoid.function(-x)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.tanh(x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh.function(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x * (x > 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return x >= 0\n",
    "\n",
    "class Linear :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return np.ones((x.shape[0], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return np.mean(np.sum((y_hat.T - y_real.T)**2, axis = 0))    \n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/y_real.shape[0]) * (y_hat - y_real)\n",
    "    \n",
    "class reg_MSE:\n",
    "    def __init__(self, weights, lam):\n",
    "        self.weights = weights\n",
    "        self.lam = lam\n",
    "        \n",
    "    def loss(self, y_real, y_hat):\n",
    "        loss = np.mean(np.sum((y_hat.T - y_real.T)**2, axis = 0))\n",
    "        reg = (np.sum(np.square(self.weights[1])) + np.sum(np.square(self.weights[2])))*(self.lam * self.weights[1].shape[1]/self.weights[1].shape[0])\n",
    "        return loss + reg\n",
    "        \n",
    "    def gradient(self, y_real, y_hat):\n",
    "        loss_grad = (2/y_real.shape[0]) * (y_hat - y_real)\n",
    "        reg_grad = (self.lam * self.weights[1].shape[1]/self.weights[1].shape[0]) * (np.sum(self.weights[1]) + np.sum(self.weights[2]))\n",
    "        return loss_grad + reg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "                \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "                \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(1, self.nb_layers):\n",
    "            self.weights[i] = np.random.randn(self.dimensions[i - 1], self.dimensions[i]) / np.sqrt(self.dimensions[i - 1])\n",
    "            self.bias[i] = np.zeros(self.dimensions[i])\n",
    "         \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \n",
    "        return : z contient les paramètres avant que l'on applique l'activation function\n",
    "        return : a contient les paramètres après que l'on applique l'activation function\n",
    "        \"\"\"\n",
    "        z = {}\n",
    "        a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        for i in range(1, self.nb_layers):\n",
    "            z[i + 1] = np.dot(a[i], self.weights[i]) + self.bias[i] #Z = XW + b\n",
    "            a[i + 1] = self.activations[i + 1].function(z[i + 1])\n",
    "            \n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, y_real):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(y_real, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = np.dot(a[self.nb_layers - 1].T, delta)\n",
    "     \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'hidden layer\n",
    "        delta = np.dot(delta, self.weights[2].T) * self.activations[2].gradient(z[2])\n",
    "        partial_deriv = np.dot(a[1].T, delta) \n",
    "        update_parameters[1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "\n",
    "    def fit(self, x, y_real, x_test, y_test, loss, nb_iterations = 100, batch_size = 100, learning_rate1 = 0, learning_rate2 = 0.3):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "            \n",
    "        loss_tab = []\n",
    "        \n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            _, a_train = self.forward_pass(x)\n",
    "            _, a_test = self.forward_pass(x_test)\n",
    "            loss_tab.append({\n",
    "                \"train_mse\":  self.loss_function.loss(y_real, a_train[self.nb_layers]),\n",
    "                \"test_mse\":  self.loss_function.loss(y_test, a_test[self.nb_layers]),\n",
    "            })\n",
    "            if(i % 100) == 0:\n",
    "                print(f\"Loss at Iteration {i} for first batch is {loss_tab[-1]}\")\n",
    "        return loss_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = 100\n",
    "n_train = 300\n",
    "n_test = 300\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "nn2 = Network(100, Sigmoid, Linear)\n",
    "config = dict(\n",
    "        nb_iterations=10000, \n",
    "        batch_size=300, \n",
    "        learning_rate1=0, \n",
    "        learning_rate2=0.005\n",
    "    )\n",
    "\n",
    "loss_func = reg_MSE(nn2.weights, 0)\n",
    "history = nn2.fit(x_train, y_train, x_test, y_test, loss_func, **config)\n",
    "\n",
    "test_prediction2 = nn2.predict(x_test)\n",
    "print(test_prediction2)\n",
    "\n",
    "nn = Network(100, Sigmoid, Linear)\n",
    "config = dict(\n",
    "        nb_iterations=10000, \n",
    "        batch_size=300, \n",
    "        learning_rate1=0, \n",
    "        learning_rate2=0.005\n",
    "    )\n",
    "\n",
    "history = nn.fit(x_train, y_train, x_test, y_test, MSE, **config)\n",
    "test_prediction1 = nn.predict(x_test)\n",
    "print(test_prediction1)\n",
    "\n",
    "print(MSE.loss(test_prediction1, test_prediction2))\n",
    "\n",
    "#print(y_test[0])\n",
    "\n",
    "#beta_calc = np.dot(np.dot(np.linalg.inv(np.dot(x_train.T, x_train)), x_train.T), y_train)\n",
    "#real_test_prediction = np.dot(beta_calc.T, x_test.T)\n",
    "#print(\"#########################\")\n",
    "#print(real_test_prediction)\n",
    "#print(MSE.loss(test_prediction, real_test_prediction.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress 0.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.0924414133319087, 'test_mse': 1.1794905157024478}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.9758447714395696, 'test_mse': 1.116053508642046}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.9230416587576421, 'test_mse': 1.084024560686485}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.8802717209099554, 'test_mse': 1.0542566703453036}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.8442822645981265, 'test_mse': 1.0283643938056954}\n",
      "1.0063862282104064\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4782294253311397, 'test_mse': 1.6527077366537017}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.1323939952146358, 'test_mse': 1.2107040575699113}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 1.0430738474519379, 'test_mse': 1.1119676405427734}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.9765872482415553, 'test_mse': 1.0489271860319112}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.9219796053008372, 'test_mse': 0.9992603924100596}\n",
      "0.959012975861674\n",
      "progress 6.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1149747801946868, 'test_mse': 1.2433335982757792}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.0126521165943239, 'test_mse': 1.156854009833922}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.9337080987823435, 'test_mse': 1.0873751964084055}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.8712293971774684, 'test_mse': 1.0313163571938337}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.8208971529162764, 'test_mse': 0.985473081386275}\n",
      "0.9478162444851803\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.6687020619094703, 'test_mse': 1.5007770749077394}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.23844665682372, 'test_mse': 1.227204203152918}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 1.076147161880311, 'test_mse': 1.1396544232879033}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.9533935660860061, 'test_mse': 1.073661456938196}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.8589475620813998, 'test_mse': 1.024095258186514}\n",
      "0.9867468149421249\n",
      "progress 13.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4512678938954633, 'test_mse': 1.3691434249567365}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.234312848832037, 'test_mse': 1.2137830807444998}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 1.0705320760716923, 'test_mse': 1.0963868304406612}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.9450254567390717, 'test_mse': 1.0075534610462167}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.8474811777826677, 'test_mse': 0.9394567629493141}\n",
      "0.8869887904122938\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2699413974762237, 'test_mse': 1.249659829469758}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.0152306194747138, 'test_mse': 1.1423588585902331}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.8974360364336601, 'test_mse': 1.07198818118652}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.8113576735654909, 'test_mse': 1.0164201444332461}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.7463788375096758, 'test_mse': 0.972058082775968}\n",
      "0.9358511282190805\n",
      "progress 20.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 2.032498926144492, 'test_mse': 1.91078051838152}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.1864567256275889, 'test_mse': 1.2559828683023664}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.9331686371113884, 'test_mse': 1.060426902187715}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.7640141879145359, 'test_mse': 0.9279793945947461}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.6476222329176398, 'test_mse': 0.8366761370144494}\n",
      "0.7729416554223514\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2104144157434271, 'test_mse': 0.9959508842506399}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.9652624963801638, 'test_mse': 0.8768775126911862}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.8148687573956163, 'test_mse': 0.7950330510220451}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.7056379010808594, 'test_mse': 0.7384330318350201}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.6243897823244926, 'test_mse': 0.6984618627527169}\n",
      "0.6697700870432465\n",
      "progress 26.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3241172396802978, 'test_mse': 1.5210700883137291}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.0146801542033623, 'test_mse': 1.2648237861992784}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.81719728411159, 'test_mse': 1.0846160513132312}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.6821544733235982, 'test_mse': 0.9573756169326745}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.5863603184920163, 'test_mse': 0.8644605027244758}\n",
      "0.7950963559217473\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.370597352959361, 'test_mse': 1.4755853420657483}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.025602440234925, 'test_mse': 1.2060051803880318}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.8141687359557324, 'test_mse': 1.0160790639146258}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.6672749449114356, 'test_mse': 0.8801329247891116}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.5622475170480324, 'test_mse': 0.7805547762209156}\n",
      "0.70667185358574\n",
      "progress 33.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2319279180093416, 'test_mse': 1.3859616037909055}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.9399365626610434, 'test_mse': 1.1529647815320725}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.7544765916056243, 'test_mse': 1.0024936034396001}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.6307006439083083, 'test_mse': 0.8989423773471062}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.5442132691905003, 'test_mse': 0.8242466842746503}\n",
      "0.7686254294096392\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4999920045174944, 'test_mse': 1.1147676090488579}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.9472131867720118, 'test_mse': 0.8289384537722702}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.7097516326798154, 'test_mse': 0.7202329284845532}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.5602061300433818, 'test_mse': 0.6510246062252303}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.46199077796009325, 'test_mse': 0.6045562417496999}\n",
      "0.5719565795405289\n",
      "progress 40.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3666549977888076, 'test_mse': 1.2845008176955606}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.9561804231350851, 'test_mse': 0.9767626756108931}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.7446212083892968, 'test_mse': 0.8131180087754099}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.6030680913452251, 'test_mse': 0.699648703931242}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.5035801348636366, 'test_mse': 0.6177646286519536}\n",
      "0.5572476390887001\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.689501370178803, 'test_mse': 1.9120868191806524}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.0454946459127406, 'test_mse': 1.2955452813545123}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.7760109418528748, 'test_mse': 1.0520264304193898}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.6035566235650681, 'test_mse': 0.894333423364803}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.48864085141687713, 'test_mse': 0.7878515056769385}\n",
      "0.7135879026012963\n",
      "progress 46.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4143578763425455, 'test_mse': 1.2310537323218864}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.8964448074247434, 'test_mse': 0.9440850527785166}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.6309398118642991, 'test_mse': 0.7781548589261663}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.4744699673691559, 'test_mse': 0.6805049421448862}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 400 for first batch is {'train_mse': 0.3773750893634397, 'test_mse': 0.6193806434007899}\n",
      "0.5789063740083854\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.7460039023130904, 'test_mse': 1.2894063875323187}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 1.1329429017016832, 'test_mse': 0.9598284290153877}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.7978213274963485, 'test_mse': 0.7699711166177312}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.6022014184520093, 'test_mse': 0.6563981650035364}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.4811809560380783, 'test_mse': 0.5832085158036087}\n",
      "0.5330072898607163\n",
      "progress 53.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3475374011797148, 'test_mse': 1.3442844879683251}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.8811894920869154, 'test_mse': 1.0048854942733327}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.6296240434125161, 'test_mse': 0.815028261173478}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.47961412053055597, 'test_mse': 0.6932546795221881}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.3846848309718119, 'test_mse': 0.6107317678642956}\n",
      "0.5527341921531268\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3000875528334752, 'test_mse': 1.1930638292913784}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.8611134236530548, 'test_mse': 0.8628717056481441}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.6306482968162215, 'test_mse': 0.6806292391547577}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.4904158954886383, 'test_mse': 0.5649223675114632}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.39932531337926025, 'test_mse': 0.48754179057768926}\n",
      "0.43389791060183425\n",
      "progress 60.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2399077237109133, 'test_mse': 1.1877314839466155}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.8146521226166642, 'test_mse': 0.9034995308043572}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.5838013112583272, 'test_mse': 0.7398637845069124}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.4467052644483712, 'test_mse': 0.6387081809156102}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.3593136296023776, 'test_mse': 0.5723413486207721}\n",
      "0.5268574846530009\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.2540569653692115, 'test_mse': 1.538661329700576}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.7782007208972572, 'test_mse': 1.0596651027688344}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.5699653712254847, 'test_mse': 0.8516680694882865}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.44178672644050543, 'test_mse': 0.7221511925010692}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.3580531843237573, 'test_mse': 0.6370128470691483}\n",
      "0.5786955079622502\n",
      "progress 66.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1391530272100876, 'test_mse': 1.361113467481958}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.7749324821581148, 'test_mse': 1.0457910459074105}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.5751700752720897, 'test_mse': 0.8555146332011357}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.44813229746006583, 'test_mse': 0.7276331287848904}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.36277014556642406, 'test_mse': 0.6373385153640214}\n",
      "0.5715450212927343\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.3913084802585691, 'test_mse': 1.5648908677291797}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.8961548081713757, 'test_mse': 1.1697169841163182}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.6513122833672026, 'test_mse': 0.9566095827355139}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.5037532697470121, 'test_mse': 0.8223527924732899}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.40705909633409154, 'test_mse': 0.7307233431493103}\n",
      "0.6647066454461479\n",
      "progress 73.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.54789061149523, 'test_mse': 1.2954056802371519}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.9375128987191579, 'test_mse': 0.9330165614755489}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.6413419796600521, 'test_mse': 0.7173861922785887}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.46831933819183574, 'test_mse': 0.582251592033879}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.35942306688391473, 'test_mse': 0.4917847732951009}\n",
      "0.42888066827608656\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.41481949723185, 'test_mse': 1.4646009880732989}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.7935153925505307, 'test_mse': 0.9971709627101942}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.5187724172319192, 'test_mse': 0.7774733825021425}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.37245854621880614, 'test_mse': 0.6474890127549768}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.2878539250253994, 'test_mse': 0.563626378760825}\n",
      "0.5061521972075858\n",
      "progress 80.00%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.403764084047865, 'test_mse': 1.2973582030350463}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.8132156772648342, 'test_mse': 0.9654506924619595}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.539101000712509, 'test_mse': 0.777386975441185}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.3896798535632825, 'test_mse': 0.6640492060719241}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.30053392905549625, 'test_mse': 0.5884717245009625}\n",
      "0.53462868339772\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4908732071741322, 'test_mse': 1.4646733617490957}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.8383466327379686, 'test_mse': 0.9620288357383686}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.5621259653474525, 'test_mse': 0.7303695185192546}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.407117941137108, 'test_mse': 0.59319701904354}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.3119491204524008, 'test_mse': 0.5056113175737732}\n",
      "0.4467619445395384\n",
      "progress 86.67%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4850442299008573, 'test_mse': 1.61485936686954}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.683155101019664, 'test_mse': 0.9944534569762244}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.4237102669597563, 'test_mse': 0.7707018216824676}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.29904608005220096, 'test_mse': 0.6465915150653967}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.2301109144721308, 'test_mse': 0.5687394567376614}\n",
      "0.5157770666271021\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.4345709390587187, 'test_mse': 1.364718495591134}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.7733220628913963, 'test_mse': 0.7859841403499933}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.5023970577510751, 'test_mse': 0.5632101961922704}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.3592671247168922, 'test_mse': 0.44089241734087276}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.27443794372612557, 'test_mse': 0.36726119923567163}\n",
      "0.32009710878371794\n",
      "progress 93.33%\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.5881476009565862, 'test_mse': 1.5957951544354934}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.793360287807357, 'test_mse': 1.0254161283897008}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.5092046922755787, 'test_mse': 0.7862800335177808}\n",
      "Loss at Iteration 300 for first batch is {'train_mse': 0.36034168724364446, 'test_mse': 0.642219854171993}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.27250525657901364, 'test_mse': 0.5469018620969227}\n",
      "0.4803757878916972\n",
      "Loss at Iteration 0 for first batch is {'train_mse': 1.1975395916009608, 'test_mse': 1.0710209541555884}\n",
      "Loss at Iteration 100 for first batch is {'train_mse': 0.648722205701011, 'test_mse': 0.8081602311179497}\n",
      "Loss at Iteration 200 for first batch is {'train_mse': 0.4385359718145543, 'test_mse': 0.6624178137703329}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 300 for first batch is {'train_mse': 0.32190526854956064, 'test_mse': 0.5694999165677888}\n",
      "Loss at Iteration 400 for first batch is {'train_mse': 0.2504043753517213, 'test_mse': 0.5044426084859048}\n",
      "0.45650109101273634\n"
     ]
    }
   ],
   "source": [
    "d = 100\n",
    "n_train = 300\n",
    "n_test = 300\n",
    "\n",
    "N_SPACE = np.arange(50, 500, 30)\n",
    "\n",
    "l = []\n",
    "\n",
    "for i, N in enumerate(N_SPACE):\n",
    "  print(f\"progress {100*i/len(N_SPACE):.2f}%\")\n",
    "  l0 = []\n",
    "  for _ in range(5):\n",
    "    x_train, y_train, x_test, y_test = get_dataset(d, n_train, n_test)\n",
    "\n",
    "    nn = Network(N, Relu, Linear)\n",
    "    config = dict(\n",
    "        nb_iterations=500, \n",
    "        batch_size=300, \n",
    "        learning_rate1=0.00, \n",
    "        #learning_rate2=0.01 * (d / N)\n",
    "        learning_rate2=0.001\n",
    "    )\n",
    "    loss_func = reg_MSE(nn.weights, 0)\n",
    "    history = nn.fit(x_train, y_train, x_test, y_test, loss_func, **config)\n",
    "    test_prediction = nn.predict(x_test)\n",
    "    final_test_error = nn.loss_function.loss(y_test, test_prediction)\n",
    "    l0.append(final_test_error)\n",
    "    print(final_test_error)\n",
    "  l0 = np.array(l0)\n",
    "  l.append({\"mean\": l0.mean(), \"std\": l0.std()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(l)\n",
    "df.plot(grid=True)\n",
    "mpl.ylim(-0.1, 5)\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.982700</td>\n",
       "      <td>0.023687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.967282</td>\n",
       "      <td>0.019465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.911420</td>\n",
       "      <td>0.024431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.721356</td>\n",
       "      <td>0.051586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.750884</td>\n",
       "      <td>0.044212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.670291</td>\n",
       "      <td>0.098334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.635418</td>\n",
       "      <td>0.078170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.555957</td>\n",
       "      <td>0.022950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.493316</td>\n",
       "      <td>0.059418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.552776</td>\n",
       "      <td>0.025919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.618126</td>\n",
       "      <td>0.046581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.467516</td>\n",
       "      <td>0.038636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.490695</td>\n",
       "      <td>0.043933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.417937</td>\n",
       "      <td>0.097840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.468438</td>\n",
       "      <td>0.011937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean       std\n",
       "0   0.982700  0.023687\n",
       "1   0.967282  0.019465\n",
       "2   0.911420  0.024431\n",
       "3   0.721356  0.051586\n",
       "4   0.750884  0.044212\n",
       "5   0.670291  0.098334\n",
       "6   0.635418  0.078170\n",
       "7   0.555957  0.022950\n",
       "8   0.493316  0.059418\n",
       "9   0.552776  0.025919\n",
       "10  0.618126  0.046581\n",
       "11  0.467516  0.038636\n",
       "12  0.490695  0.043933\n",
       "13  0.417937  0.097840\n",
       "14  0.468438  0.011937"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train_mse': 1.1975395916009608, 'test_mse': 1.0710209541555884},\n",
       " {'train_mse': 1.1576700450452697, 'test_mse': 1.0519885439178207},\n",
       " {'train_mse': 1.127386457555904, 'test_mse': 1.03970531130827},\n",
       " {'train_mse': 1.10398586597131, 'test_mse': 1.0319036689433956},\n",
       " {'train_mse': 1.0855313964962616, 'test_mse': 1.0270142950882757},\n",
       " {'train_mse': 1.070635095020131, 'test_mse': 1.023959670193027},\n",
       " {'train_mse': 1.0583023207787554, 'test_mse': 1.0220074488622435},\n",
       " {'train_mse': 1.0478202513542882, 'test_mse': 1.0206665045775951},\n",
       " {'train_mse': 1.03867799431026, 'test_mse': 1.0196133805302703},\n",
       " {'train_mse': 1.0305093455981562, 'test_mse': 1.018640383309962},\n",
       " {'train_mse': 1.0230517748182244, 'test_mse': 1.0176190624516792},\n",
       " {'train_mse': 1.016117037338428, 'test_mse': 1.0164746112414884},\n",
       " {'train_mse': 1.009570117284525, 'test_mse': 1.0151680056095445},\n",
       " {'train_mse': 1.0033141397622258, 'test_mse': 1.0136836136831362},\n",
       " {'train_mse': 0.9972795601509263, 'test_mse': 1.0120206626694508},\n",
       " {'train_mse': 0.9914164180029832, 'test_mse': 1.0101874166739557},\n",
       " {'train_mse': 0.9856887867929337, 'test_mse': 1.0081972521584726},\n",
       " {'train_mse': 0.9800707970363124, 'test_mse': 1.0060660551692333},\n",
       " {'train_mse': 0.9745437867587416, 'test_mse': 1.0038105335314997},\n",
       " {'train_mse': 0.9690942597337285, 'test_mse': 1.0014471574541286},\n",
       " {'train_mse': 0.9637124225027469, 'test_mse': 0.9989915273909932},\n",
       " {'train_mse': 0.9583911361043477, 'test_mse': 0.9964580285597443},\n",
       " {'train_mse': 0.9531251649506082, 'test_mse': 0.9938596743645747},\n",
       " {'train_mse': 0.9479106386156684, 'test_mse': 0.9912080712112558},\n",
       " {'train_mse': 0.9427446661801557, 'test_mse': 0.9885134584831909},\n",
       " {'train_mse': 0.9376250598850864, 'test_mse': 0.9857847923664586},\n",
       " {'train_mse': 0.9325501371083624, 'test_mse': 0.9830298526234879},\n",
       " {'train_mse': 0.9275185784611327, 'test_mse': 0.980255358639616},\n",
       " {'train_mse': 0.9225293260953489, 'test_mse': 0.9774670860439229},\n",
       " {'train_mse': 0.9175815108236183, 'test_mse': 0.9746699786031274},\n",
       " {'train_mse': 0.912674399883839, 'test_mse': 0.9718682523786998},\n",
       " {'train_mse': 0.907807359496411, 'test_mse': 0.9690654906581525},\n",
       " {'train_mse': 0.902979828020821, 'test_mse': 0.9662647291587185},\n",
       " {'train_mse': 0.8981912967070673, 'test_mse': 0.9634685316222938},\n",
       " {'train_mse': 0.8934412958891318, 'test_mse': 0.9606790562918195},\n",
       " {'train_mse': 0.888729385077966, 'test_mse': 0.9578981139631717},\n",
       " {'train_mse': 0.8840551458487412, 'test_mse': 0.9551272184001167},\n",
       " {'train_mse': 0.8794181767304254, 'test_mse': 0.952367629922307},\n",
       " {'train_mse': 0.874818089530242, 'test_mse': 0.9496203929546887},\n",
       " {'train_mse': 0.8702545066864292, 'test_mse': 0.9468863682792533},\n",
       " {'train_mse': 0.8657270593579699, 'test_mse': 0.9441662606690778},\n",
       " {'train_mse': 0.861235386042545, 'test_mse': 0.9414606425180118},\n",
       " {'train_mse': 0.856779131573144, 'test_mse': 0.9387699740123012},\n",
       " {'train_mse': 0.85235794638615, 'test_mse': 0.9360946203259806},\n",
       " {'train_mse': 0.8479714859841156, 'test_mse': 0.9334348662618103},\n",
       " {'train_mse': 0.8436194105381943, 'test_mse': 0.9307909287047589},\n",
       " {'train_mse': 0.8393013845908025, 'test_mse': 0.928162967205828},\n",
       " {'train_mse': 0.8350170768302553, 'test_mse': 0.9255510929703505},\n",
       " {'train_mse': 0.8307661599171315, 'test_mse': 0.9229553764864814},\n",
       " {'train_mse': 0.8265483103478547, 'test_mse': 0.9203758539960419},\n",
       " {'train_mse': 0.8223632083450962, 'test_mse': 0.9178125329807182},\n",
       " {'train_mse': 0.818210537767545, 'test_mse': 0.9152653968114216},\n",
       " {'train_mse': 0.8140899860337028, 'test_mse': 0.9127344086868769},\n",
       " {'train_mse': 0.8100012440558754, 'test_mse': 0.9102195149688581},\n",
       " {'train_mse': 0.8059440061816138, 'test_mse': 0.9077206480054838},\n",
       " {'train_mse': 0.8019179701406373, 'test_mse': 0.90523772852032},\n",
       " {'train_mse': 0.7979228369958219, 'test_mse': 0.9027706676333419},\n",
       " {'train_mse': 0.7939583110972448, 'test_mse': 0.9003193685698598},\n",
       " {'train_mse': 0.7900241000385516, 'test_mse': 0.8978837281050228},\n",
       " {'train_mse': 0.786119914615126, 'test_mse': 0.8954636377843008},\n",
       " {'train_mse': 0.7822454687836826, 'test_mse': 0.893058984954213},\n",
       " {'train_mse': 0.7784004796230118, 'test_mse': 0.8906696536323454},\n",
       " {'train_mse': 0.7745846672956804, 'test_mse': 0.8882955252412915},\n",
       " {'train_mse': 0.7707977550105435, 'test_mse': 0.8859364792273772},\n",
       " {'train_mse': 0.7670394689859674, 'test_mse': 0.883592393581852},\n",
       " {'train_mse': 0.7633095384136834, 'test_mse': 0.8812631452795308},\n",
       " {'train_mse': 0.7596076954232196, 'test_mse': 0.8789486106475638},\n",
       " {'train_mse': 0.7559336750468659, 'test_mse': 0.8766486656750893},\n",
       " {'train_mse': 0.7522872151851443, 'test_mse': 0.8743631862728621},\n",
       " {'train_mse': 0.7486680565727566, 'test_mse': 0.8720920484905665},\n",
       " {'train_mse': 0.7450759427449923, 'test_mse': 0.8698351286983343},\n",
       " {'train_mse': 0.7415106200045826, 'test_mse': 0.8675923037379953},\n",
       " {'train_mse': 0.7379718373889859, 'test_mse': 0.8653634510487348},\n",
       " {'train_mse': 0.7344593466380961, 'test_mse': 0.8631484487711119},\n",
       " {'train_mse': 0.7309729021623658, 'test_mse': 0.8609471758327993},\n",
       " {'train_mse': 0.727512261011332, 'test_mse': 0.8587595120188681},\n",
       " {'train_mse': 0.7240771828425446, 'test_mse': 0.8565853380290317},\n",
       " {'train_mse': 0.7206674298908838, 'test_mse': 0.854424535523869},\n",
       " {'train_mse': 0.7172827669382656, 'test_mse': 0.852276987161753},\n",
       " {'train_mse': 0.7139229612837273, 'test_mse': 0.8501425766279384},\n",
       " {'train_mse': 0.7105877827138884, 'test_mse': 0.8480211886570385},\n",
       " {'train_mse': 0.7072770034737811, 'test_mse': 0.8459127090499359},\n",
       " {'train_mse': 0.7039903982380461, 'test_mse': 0.8438170246860047},\n",
       " {'train_mse': 0.7007277440824881, 'test_mse': 0.8417340235313946},\n",
       " {'train_mse': 0.6974888204559865, 'test_mse': 0.8396635946440041},\n",
       " {'train_mse': 0.6942734091527566, 'test_mse': 0.837605628175679},\n",
       " {'train_mse': 0.6910812942849559, 'test_mse': 0.8355600153720867},\n",
       " {'train_mse': 0.6879122622556328, 'test_mse': 0.833526648570648},\n",
       " {'train_mse': 0.6847661017320117, 'test_mse': 0.8315054211968493},\n",
       " {'train_mse': 0.6816426036191092, 'test_mse': 0.8294962277592096},\n",
       " {'train_mse': 0.6785415610336805, 'test_mse': 0.8274989638431286},\n",
       " {'train_mse': 0.6754627692784874, 'test_mse': 0.825513526103817},\n",
       " {'train_mse': 0.6724060258168869, 'test_mse': 0.8235398122584698},\n",
       " {'train_mse': 0.6693711302477361, 'test_mse': 0.8215777210778206},\n",
       " {'train_mse': 0.6663578842806055, 'test_mse': 0.8196271523771981},\n",
       " {'train_mse': 0.6633660917113023, 'test_mse': 0.8176880070071818},\n",
       " {'train_mse': 0.6603955583976958, 'test_mse': 0.8157601868439399},\n",
       " {'train_mse': 0.6574460922358418, 'test_mse': 0.8138435947793203},\n",
       " {'train_mse': 0.6545175031364028, 'test_mse': 0.8119381347107556},\n",
       " {'train_mse': 0.6516096030013596, 'test_mse': 0.81004371153103},\n",
       " {'train_mse': 0.648722205701011, 'test_mse': 0.8081602311179497},\n",
       " {'train_mse': 0.645855127051257, 'test_mse': 0.8062876003239562},\n",
       " {'train_mse': 0.6430081847911627, 'test_mse': 0.8044257269657065},\n",
       " {'train_mse': 0.640181198560799, 'test_mse': 0.8025745198136481},\n",
       " {'train_mse': 0.6373739898793565, 'test_mse': 0.8007338885816101},\n",
       " {'train_mse': 0.6345863821235279, 'test_mse': 0.7989037439164254},\n",
       " {'train_mse': 0.6318182005061593, 'test_mse': 0.7970839973876003},\n",
       " {'train_mse': 0.6290692720551608, 'test_mse': 0.7952745614770419},\n",
       " {'train_mse': 0.6263394255926787, 'test_mse': 0.7934753495688553},\n",
       " {'train_mse': 0.6236284917145234, 'test_mse': 0.7916862759392154},\n",
       " {'train_mse': 0.6209363027698488, 'test_mse': 0.7899072557463235},\n",
       " {'train_mse': 0.618262692841082, 'test_mse': 0.7881382050204505},\n",
       " {'train_mse': 0.6156074977241001, 'test_mse': 0.7863790406540754},\n",
       " {'train_mse': 0.6129705549086476, 'test_mse': 0.7846296803921167},\n",
       " {'train_mse': 0.610351703558996, 'test_mse': 0.782890042822265},\n",
       " {'train_mse': 0.6077507844948387, 'test_mse': 0.7811600473654162},\n",
       " {'train_mse': 0.6051676401724209, 'test_mse': 0.7794396142662045},\n",
       " {'train_mse': 0.6026021146658989, 'test_mse': 0.7777286645836424},\n",
       " {'train_mse': 0.6000540536489286, 'test_mse': 0.7760271201818615},\n",
       " {'train_mse': 0.5975233043764776, 'test_mse': 0.7743349037209597},\n",
       " {'train_mse': 0.5950097156668598, 'test_mse': 0.7726519386479513},\n",
       " {'train_mse': 0.5925131378839897, 'test_mse': 0.7709781491878228},\n",
       " {'train_mse': 0.5900334229198517, 'test_mse': 0.7693134603346913},\n",
       " {'train_mse': 0.5875704241771844, 'test_mse': 0.7676577978430671},\n",
       " {'train_mse': 0.5851239965523743, 'test_mse': 0.7660110882192193},\n",
       " {'train_mse': 0.5826939964185596, 'test_mse': 0.7643732587126427},\n",
       " {'train_mse': 0.5802802816089355, 'test_mse': 0.7627442373076277},\n",
       " {'train_mse': 0.5778827114002683, 'test_mse': 0.7611239527149293},\n",
       " {'train_mse': 0.5755011464966038, 'test_mse': 0.7595123343635354},\n",
       " {'train_mse': 0.5731354490131773, 'test_mse': 0.7579093123925356},\n",
       " {'train_mse': 0.5707854824605165, 'test_mse': 0.7563148176430861},\n",
       " {'train_mse': 0.5684511117287385, 'test_mse': 0.7547287816504717},\n",
       " {'train_mse': 0.5661322030720358, 'test_mse': 0.7531511366362641},\n",
       " {'train_mse': 0.5638286240933514, 'test_mse': 0.7515818155005721},\n",
       " {'train_mse': 0.5615402437292378, 'test_mse': 0.7500207518143899},\n",
       " {'train_mse': 0.5592669322349, 'test_mse': 0.7484678798120329},\n",
       " {'train_mse': 0.5570085611694184, 'test_mse': 0.7469231343836679},\n",
       " {'train_mse': 0.5547650033811512, 'test_mse': 0.7453864510679326},\n",
       " {'train_mse': 0.5525361329933116, 'test_mse': 0.7438577660446439},\n",
       " {'train_mse': 0.5503218253897197, 'test_mse': 0.7423370161275948},\n",
       " {'train_mse': 0.5481219572007262, 'test_mse': 0.7408241387574381},\n",
       " {'train_mse': 0.5459364062893042, 'test_mse': 0.7393190719946554},\n",
       " {'train_mse': 0.5437650517373097, 'test_mse': 0.7378217545126126},\n",
       " {'train_mse': 0.5416077738319073, 'test_mse': 0.7363321255906964},\n",
       " {'train_mse': 0.5394644540521584, 'test_mse': 0.7348501251075376},\n",
       " {'train_mse': 0.53733497505577, 'test_mse': 0.7333756935343115},\n",
       " {'train_mse': 0.5352192206660032, 'test_mse': 0.7319087719281231},\n",
       " {'train_mse': 0.5331170758587395, 'test_mse': 0.7304493019254689},\n",
       " {'train_mse': 0.5310284267496995, 'test_mse': 0.7289972257357795},\n",
       " {'train_mse': 0.5289531605818176, 'test_mse': 0.7275524861350388},\n",
       " {'train_mse': 0.5268911657127656, 'test_mse': 0.7261150264594801},\n",
       " {'train_mse': 0.5248423316026266, 'test_mse': 0.7246847905993588},\n",
       " {'train_mse': 0.5228065488017157, 'test_mse': 0.723261722992798},\n",
       " {'train_mse': 0.5207837089385464, 'test_mse': 0.7218457686197107},\n",
       " {'train_mse': 0.5187737047079405, 'test_mse': 0.720436872995792},\n",
       " {'train_mse': 0.5167764298592796, 'test_mse': 0.7190349821665856},\n",
       " {'train_mse': 0.5147917791848969, 'test_mse': 0.7176400427016201},\n",
       " {'train_mse': 0.5128196485086071, 'test_mse': 0.7162520016886162},\n",
       " {'train_mse': 0.5108599346743726, 'test_mse': 0.714870806727763},\n",
       " {'train_mse': 0.5089125355351048, 'test_mse': 0.7134964059260627},\n",
       " {'train_mse': 0.5069773499415985, 'test_mse': 0.712128747891742},\n",
       " {'train_mse': 0.5050542777315967, 'test_mse': 0.7107677817287331},\n",
       " {'train_mse': 0.5031432197189872, 'test_mse': 0.7094134570312167},\n",
       " {'train_mse': 0.5012440776831252, 'test_mse': 0.708065723878233},\n",
       " {'train_mse': 0.4993567543582831, 'test_mse': 0.7067245328283558},\n",
       " {'train_mse': 0.4974811534232267, 'test_mse': 0.7053898349144299},\n",
       " {'train_mse': 0.4956171794909119, 'test_mse': 0.7040615816383717},\n",
       " {'train_mse': 0.4937647380983058, 'test_mse': 0.7027397249660314},\n",
       " {'train_mse': 0.4919237356963267, 'test_mse': 0.7014242173221157},\n",
       " {'train_mse': 0.4900940796399035, 'test_mse': 0.7001150115851719},\n",
       " {'train_mse': 0.4882756781781515, 'test_mse': 0.6988120610826309},\n",
       " {'train_mse': 0.48646844044466564, 'test_mse': 0.6975153195859088},\n",
       " {'train_mse': 0.4846722764479263, 'test_mse': 0.6962247413055671},\n",
       " {'train_mse': 0.48288709706181926, 'test_mse': 0.6949402808865297},\n",
       " {'train_mse': 0.4811128140162668, 'test_mse': 0.6936618934033568},\n",
       " {'train_mse': 0.4793493398879692, 'test_mse': 0.6923895343555754},\n",
       " {'train_mse': 0.4775965880912537, 'test_mse': 0.6911231596630629},\n",
       " {'train_mse': 0.4758544728690337, 'test_mse': 0.6898627256614879},\n",
       " {'train_mse': 0.4741229092838707, 'test_mse': 0.6886081890978026},\n",
       " {'train_mse': 0.47240181320914376, 'test_mse': 0.68735950712579},\n",
       " {'train_mse': 0.4706911013203203, 'test_mse': 0.6861166373016621},\n",
       " {'train_mse': 0.46899069108633074, 'test_mse': 0.6848795375797114},\n",
       " {'train_mse': 0.4673005007610427, 'test_mse': 0.683648166308013},\n",
       " {'train_mse': 0.465620449374836, 'test_mse': 0.6824224822241755},\n",
       " {'train_mse': 0.46395045672627533, 'test_mse': 0.6812024444511459},\n",
       " {'train_mse': 0.4622904433738799, 'test_mse': 0.6799880124930598},\n",
       " {'train_mse': 0.46064033062799015, 'test_mse': 0.678779146231142},\n",
       " {'train_mse': 0.45900004054272775, 'test_mse': 0.6775758059196549},\n",
       " {'train_mse': 0.4573694959080507, 'test_mse': 0.6763779521818958},\n",
       " {'train_mse': 0.4557486202419, 'test_mse': 0.6751855460062374},\n",
       " {'train_mse': 0.4541373377824375, 'test_mse': 0.6739985487422194},\n",
       " {'train_mse': 0.45253557348037493, 'test_mse': 0.6728169220966819},\n",
       " {'train_mse': 0.45094325299139065, 'test_mse': 0.6716406281299456},\n",
       " {'train_mse': 0.4493603026686355, 'test_mse': 0.6704696292520375},\n",
       " {'train_mse': 0.447786649555325, 'test_mse': 0.6693038882189581},\n",
       " {'train_mse': 0.4462222213774172, 'test_mse': 0.6681433681289949},\n",
       " {'train_mse': 0.4446669465363757, 'test_mse': 0.666988032419077},\n",
       " {'train_mse': 0.4431207541020161, 'test_mse': 0.6658378448611728},\n",
       " {'train_mse': 0.4415835738054359, 'test_mse': 0.6646927695587291},\n",
       " {'train_mse': 0.44005533603202424, 'test_mse': 0.6635527709431531},\n",
       " {'train_mse': 0.4385359718145543, 'test_mse': 0.6624178137703329},\n",
       " {'train_mse': 0.4370254128263538, 'test_mse': 0.6612878631172006},\n",
       " {'train_mse': 0.43552359137455615, 'test_mse': 0.6601628843783338},\n",
       " {'train_mse': 0.43403044039342603, 'test_mse': 0.6590428432625972},\n",
       " {'train_mse': 0.43254589343776606, 'test_mse': 0.6579277057898225},\n",
       " {'train_mse': 0.43106988467639507, 'test_mse': 0.6568174382875283},\n",
       " {'train_mse': 0.42960234888570425, 'test_mse': 0.6557120073876751},\n",
       " {'train_mse': 0.4281432214432858, 'test_mse': 0.6546113800234603},\n",
       " {'train_mse': 0.4266924383216353, 'test_mse': 0.6535155234261499},\n",
       " {'train_mse': 0.4252499360819256, 'test_mse': 0.6524244051219451},\n",
       " {'train_mse': 0.4238156518678527, 'test_mse': 0.6513379929288866},\n",
       " {'train_mse': 0.42238952339955205, 'test_mse': 0.6502562549537941},\n",
       " {'train_mse': 0.42097148896758335, 'test_mse': 0.6491791595892411},\n",
       " {'train_mse': 0.4195614874269855, 'test_mse': 0.6481066755105641},\n",
       " {'train_mse': 0.4181594581913983, 'test_mse': 0.6470387716729065},\n",
       " {'train_mse': 0.4167653412272521, 'test_mse': 0.645975417308297},\n",
       " {'train_mse': 0.41537907704802285, 'test_mse': 0.6449165819227598},\n",
       " {'train_mse': 0.4140006067085529, 'test_mse': 0.6438622352934605},\n",
       " {'train_mse': 0.41262987179943694, 'test_mse': 0.6428123474658821},\n",
       " {'train_mse': 0.4112668144414708, 'test_mse': 0.6417668887510349},\n",
       " {'train_mse': 0.40991137728016414, 'test_mse': 0.6407258297226982},\n",
       " {'train_mse': 0.408563503480315, 'test_mse': 0.6396891412146929},\n",
       " {'train_mse': 0.4072231367206461, 'test_mse': 0.6386567943181858},\n",
       " {'train_mse': 0.4058902211885018, 'test_mse': 0.6376287603790242},\n",
       " {'train_mse': 0.40456470157460506, 'test_mse': 0.6366050109951018},\n",
       " {'train_mse': 0.40324652306787406, 'test_mse': 0.6355855180137531},\n",
       " {'train_mse': 0.40193563135029714, 'test_mse': 0.634570253529179},\n",
       " {'train_mse': 0.4006319725918655, 'test_mse': 0.6335591898799008},\n",
       " {'train_mse': 0.3993354934455638, 'test_mse': 0.6325522996462439},\n",
       " {'train_mse': 0.3980461410424162, 'test_mse': 0.6315495556478482},\n",
       " {'train_mse': 0.39676386298658933, 'test_mse': 0.63055093094121},\n",
       " {'train_mse': 0.39548860735054964, 'test_mse': 0.6295563988172482},\n",
       " {'train_mse': 0.3942203226702754, 'test_mse': 0.6285659327989012},\n",
       " {'train_mse': 0.39295895794052316, 'test_mse': 0.627579506638748},\n",
       " {'train_mse': 0.39170446261014596, 'test_mse': 0.6265970943166596},\n",
       " {'train_mse': 0.39045678657746613, 'test_mse': 0.6256186700374738},\n",
       " {'train_mse': 0.3892158801856977, 'test_mse': 0.6246442082286984},\n",
       " {'train_mse': 0.3879816942184223, 'test_mse': 0.6236736835382389},\n",
       " {'train_mse': 0.386754179895114, 'test_mse': 0.6227070708321535},\n",
       " {'train_mse': 0.3855332888667151, 'test_mse': 0.6217443451924309},\n",
       " {'train_mse': 0.38431897321126135, 'test_mse': 0.6207854819147961},\n",
       " {'train_mse': 0.38311118542955563, 'test_mse': 0.6198304565065381},\n",
       " {'train_mse': 0.3819098784408904, 'test_mse': 0.6188792446843646},\n",
       " {'train_mse': 0.38071500557881727, 'test_mse': 0.6179318223722784},\n",
       " {'train_mse': 0.3795265205869643, 'test_mse': 0.6169881656994789},\n",
       " {'train_mse': 0.3783443776148999, 'test_mse': 0.6160482509982877},\n",
       " {'train_mse': 0.37716853121404165, 'test_mse': 0.6151120548020963},\n",
       " {'train_mse': 0.37599893633361225, 'test_mse': 0.6141795538433363},\n",
       " {'train_mse': 0.37483554831663884, 'test_mse': 0.6132507250514749},\n",
       " {'train_mse': 0.3736783228959979, 'test_mse': 0.61232554555103},\n",
       " {'train_mse': 0.37252721619050333, 'test_mse': 0.6114039926596084},\n",
       " {'train_mse': 0.37138218470103784, 'test_mse': 0.6104860438859667},\n",
       " {'train_mse': 0.3702431853067275, 'test_mse': 0.6095716769280933},\n",
       " {'train_mse': 0.3691101752611587, 'test_mse': 0.6086608696713106},\n",
       " {'train_mse': 0.36798311218863616, 'test_mse': 0.6077536001864005},\n",
       " {'train_mse': 0.36686195408048305, 'test_mse': 0.6068498467277489},\n",
       " {'train_mse': 0.36574665929138167, 'test_mse': 0.6059495877315124},\n",
       " {'train_mse': 0.3646371865357543, 'test_mse': 0.6050528018138034},\n",
       " {'train_mse': 0.3635334948841838, 'test_mse': 0.604159467768897},\n",
       " {'train_mse': 0.3624355437598744, 'test_mse': 0.6032695645674574},\n",
       " {'train_mse': 0.3613432929351502, 'test_mse': 0.6023830713547836},\n",
       " {'train_mse': 0.36025670252799324, 'test_mse': 0.6014999674490743},\n",
       " {'train_mse': 0.35917573299861844, 'test_mse': 0.6006202323397131},\n",
       " {'train_mse': 0.3581003451460875, 'test_mse': 0.5997438456855712},\n",
       " {'train_mse': 0.35703050010495846, 'test_mse': 0.5988707873133299},\n",
       " {'train_mse': 0.355966159341973, 'test_mse': 0.5980010372158215},\n",
       " {'train_mse': 0.35490728465277943, 'test_mse': 0.5971345755503876},\n",
       " {'train_mse': 0.35385383815869165, 'test_mse': 0.5962713826372564},\n",
       " {'train_mse': 0.3528057823034834, 'test_mse': 0.5954114389579378},\n",
       " {'train_mse': 0.35176307985021765, 'test_mse': 0.5945547251536355},\n",
       " {'train_mse': 0.35072569387811037, 'test_mse': 0.5937012220236774},\n",
       " {'train_mse': 0.3496935877794289, 'test_mse': 0.5928509105239621},\n",
       " {'train_mse': 0.3486667252564229, 'test_mse': 0.592003771765424},\n",
       " {'train_mse': 0.34764507031829056, 'test_mse': 0.5911597870125134},\n",
       " {'train_mse': 0.34662858727817564, 'test_mse': 0.590318937681695},\n",
       " {'train_mse': 0.34561724075019895, 'test_mse': 0.589481205339961},\n",
       " {'train_mse': 0.34461099564652053, 'test_mse': 0.588646571703362},\n",
       " {'train_mse': 0.34360981717443506, 'test_mse': 0.5878150186355542},\n",
       " {'train_mse': 0.34261367083349736, 'test_mse': 0.5869865281463594},\n",
       " {'train_mse': 0.34162252241267965, 'test_mse': 0.5861610823903454},\n",
       " {'train_mse': 0.34063633798756, 'test_mse': 0.5853386636654174},\n",
       " {'train_mse': 0.33965508391753996, 'test_mse': 0.5845192544114276},\n",
       " {'train_mse': 0.33867872684309336, 'test_mse': 0.5837028372087992},\n",
       " {'train_mse': 0.33770723368304395, 'test_mse': 0.5828893947771644},\n",
       " {'train_mse': 0.33674057163187254, 'test_mse': 0.5820789099740185},\n",
       " {'train_mse': 0.3357787081570537, 'test_mse': 0.5812713657933878},\n",
       " {'train_mse': 0.33482161099642005, 'test_mse': 0.5804667453645121},\n",
       " {'train_mse': 0.33386924815555574, 'test_mse': 0.5796650319505419},\n",
       " {'train_mse': 0.3329215879052175, 'test_mse': 0.5788662089472483},\n",
       " {'train_mse': 0.3319785987787834, 'test_mse': 0.5780702598817491},\n",
       " {'train_mse': 0.3310402495697287, 'test_mse': 0.5772771684112463},\n",
       " {'train_mse': 0.33010650932912927, 'test_mse': 0.5764869183217793},\n",
       " {'train_mse': 0.3291773473631915, 'test_mse': 0.5756994935269897},\n",
       " {'train_mse': 0.3282527332308075, 'test_mse': 0.5749148780668999},\n",
       " {'train_mse': 0.32733263674113877, 'test_mse': 0.5741330561067073},\n",
       " {'train_mse': 0.32641702795122307, 'test_mse': 0.5733540119355861},\n",
       " {'train_mse': 0.3255058771636084, 'test_mse': 0.572577729965508},\n",
       " {'train_mse': 0.32459915492401187, 'test_mse': 0.5718041947300714},\n",
       " {'train_mse': 0.32369683201900284, 'test_mse': 0.5710333908833445},\n",
       " {'train_mse': 0.32279887947371133, 'test_mse': 0.5702653031987213},\n",
       " {'train_mse': 0.32190526854956064, 'test_mse': 0.5694999165677888},\n",
       " {'train_mse': 0.3210159707420238, 'test_mse': 0.5687372159992072},\n",
       " {'train_mse': 0.320130957778404, 'test_mse': 0.5679771866176014},\n",
       " {'train_mse': 0.3192502016156382, 'test_mse': 0.5672198136624651},\n",
       " {'train_mse': 0.3183736744381245, 'test_mse': 0.5664650824870754},\n",
       " {'train_mse': 0.31750134865557195, 'test_mse': 0.5657129785574203},\n",
       " {'train_mse': 0.31663319690087344, 'test_mse': 0.5649634874511361},\n",
       " {'train_mse': 0.31576919202800063, 'test_mse': 0.5642165948564585},\n",
       " {'train_mse': 0.31490930710992127, 'test_mse': 0.5634722865711818},\n",
       " {'train_mse': 0.3140535154365389, 'test_mse': 0.5627305485016312},\n",
       " {'train_mse': 0.31320179051265346, 'test_mse': 0.5619913666616457},\n",
       " {'train_mse': 0.31235410605594344, 'test_mse': 0.5612547271715715},\n",
       " {'train_mse': 0.31151043599496975, 'test_mse': 0.5605206162572653},\n",
       " {'train_mse': 0.31067075446720027, 'test_mse': 0.5597890202491105},\n",
       " {'train_mse': 0.30983503581705474, 'test_mse': 0.5590599255810402},\n",
       " {'train_mse': 0.3090032545939706, 'test_mse': 0.5583333187895744},\n",
       " {'train_mse': 0.308175385550489, 'test_mse': 0.5576091865128641},\n",
       " {'train_mse': 0.3073514036403605, 'test_mse': 0.5568875154897479},\n",
       " {'train_mse': 0.3065312840166707, 'test_mse': 0.5561682925588167},\n",
       " {'train_mse': 0.30571500202998547, 'test_mse': 0.5554515046574895},\n",
       " {'train_mse': 0.3049025332265162, 'test_mse': 0.5547371388210979},\n",
       " {'train_mse': 0.3040938533463022, 'test_mse': 0.5540251821819819},\n",
       " {'train_mse': 0.30328893832141424, 'test_mse': 0.5533156219685925},\n",
       " {'train_mse': 0.3024877642741755, 'test_mse': 0.5526084455046062},\n",
       " {'train_mse': 0.30169030751540143, 'test_mse': 0.5519036402080476},\n",
       " {'train_mse': 0.30089654454265724, 'test_mse': 0.5512011935904209},\n",
       " {'train_mse': 0.30010645203853453, 'test_mse': 0.5505010932558522},\n",
       " {'train_mse': 0.2993200068689449, 'test_mse': 0.5498033269002379},\n",
       " {'train_mse': 0.2985371860814313, 'test_mse': 0.5491078823104047},\n",
       " {'train_mse': 0.29775796690349743, 'test_mse': 0.5484147473632769},\n",
       " {'train_mse': 0.2969823267409536, 'test_mse': 0.5477239100250524},\n",
       " {'train_mse': 0.2962102431762803, 'test_mse': 0.5470353583503883},\n",
       " {'train_mse': 0.29544169396700815, 'test_mse': 0.5463490804815934},\n",
       " {'train_mse': 0.2946766570441148, 'test_mse': 0.5456650646478304},\n",
       " {'train_mse': 0.29391511051043817, 'test_mse': 0.5449832991643259},\n",
       " {'train_mse': 0.29315703263910653, 'test_mse': 0.5443037724315883},\n",
       " {'train_mse': 0.29240240187198363, 'test_mse': 0.5436264729346344},\n",
       " {'train_mse': 0.2916511968181308, 'test_mse': 0.5429513892422233},\n",
       " {'train_mse': 0.29090339625228445, 'test_mse': 0.542278510006099},\n",
       " {'train_mse': 0.2901589791133487, 'test_mse': 0.5416078239602398},\n",
       " {'train_mse': 0.28941792450290454, 'test_mse': 0.5409393199201169},\n",
       " {'train_mse': 0.2886802116837328, 'test_mse': 0.540272986781959},\n",
       " {'train_mse': 0.2879458200783535, 'test_mse': 0.5396088135220257},\n",
       " {'train_mse': 0.2872147292675793, 'test_mse': 0.5389467891958872},\n",
       " {'train_mse': 0.2864869189890838, 'test_mse': 0.5382869029377138},\n",
       " {'train_mse': 0.2857623691359849, 'test_mse': 0.5376291439595686},\n",
       " {'train_mse': 0.28504105975544186, 'test_mse': 0.5369735015507121},\n",
       " {'train_mse': 0.28432297104726717, 'test_mse': 0.5363199650769103},\n",
       " {'train_mse': 0.28360808336255217, 'test_mse': 0.5356685239797516},\n",
       " {'train_mse': 0.282896377202307, 'test_mse': 0.5350191677759708},\n",
       " {'train_mse': 0.28218783321611357, 'test_mse': 0.534371886056779},\n",
       " {'train_mse': 0.28148243220079316, 'test_mse': 0.5337266684872009},\n",
       " {'train_mse': 0.28078015509908694, 'test_mse': 0.5330835048054193},\n",
       " {'train_mse': 0.28008098299834955, 'test_mse': 0.5324423848221257},\n",
       " {'train_mse': 0.2793848971292566, 'test_mse': 0.5318032984198772},\n",
       " {'train_mse': 0.27869187886452446, 'test_mse': 0.5311662355524605},\n",
       " {'train_mse': 0.27800190971764344, 'test_mse': 0.5305311862442624},\n",
       " {'train_mse': 0.2773149713416237, 'test_mse': 0.5298981405896476},\n",
       " {'train_mse': 0.27663104552775325, 'test_mse': 0.5292670887523386},\n",
       " {'train_mse': 0.27595011420436955, 'test_mse': 0.528638020964809},\n",
       " {'train_mse': 0.2752721594356416, 'test_mse': 0.5280109275276754},\n",
       " {'train_mse': 0.2745971634203662, 'test_mse': 0.5273857988091017},\n",
       " {'train_mse': 0.27392510849077534, 'test_mse': 0.5267626252442047},\n",
       " {'train_mse': 0.2732559771113551, 'test_mse': 0.5261413973344687},\n",
       " {'train_mse': 0.2725897518776772, 'test_mse': 0.5255221056471652},\n",
       " {'train_mse': 0.27192641551524177, 'test_mse': 0.5249047408147771},\n",
       " {'train_mse': 0.2712659508783319, 'test_mse': 0.5242892935344311},\n",
       " {'train_mse': 0.2706083409488793, 'test_mse': 0.523675754567334},\n",
       " {'train_mse': 0.2699535688353413, 'test_mse': 0.5230641147382146},\n",
       " {'train_mse': 0.2693016177715894, 'test_mse': 0.5224543649347727},\n",
       " {'train_mse': 0.2686524711158084, 'test_mse': 0.5218464961071324},\n",
       " {'train_mse': 0.268006112349407, 'test_mse': 0.5212404992673008},\n",
       " {'train_mse': 0.26736252507593844, 'test_mse': 0.5206363654886326},\n",
       " {'train_mse': 0.26672169302003235, 'test_mse': 0.5200340859053006},\n",
       " {'train_mse': 0.26608360002633735, 'test_mse': 0.5194336517117699},\n",
       " {'train_mse': 0.26544823005847373, 'test_mse': 0.5188350541622787},\n",
       " {'train_mse': 0.26481556719799626, 'test_mse': 0.5182382845703235},\n",
       " {'train_mse': 0.2641855956433682, 'test_mse': 0.5176433343081506},\n",
       " {'train_mse': 0.26355829970894423, 'test_mse': 0.5170501948062503},\n",
       " {'train_mse': 0.2629336638239643, 'test_mse': 0.516458857552859},\n",
       " {'train_mse': 0.2623116725315569, 'test_mse': 0.5158693140934644},\n",
       " {'train_mse': 0.26169231048775216, 'test_mse': 0.5152815560303158},\n",
       " {'train_mse': 0.2610755624605052, 'test_mse': 0.5146955750219394},\n",
       " {'train_mse': 0.260461413328728, 'test_mse': 0.5141113627826592},\n",
       " {'train_mse': 0.25984984808133205, 'test_mse': 0.5135289110821213},\n",
       " {'train_mse': 0.25924085181627915, 'test_mse': 0.5129482117448236},\n",
       " {'train_mse': 0.2586344097396424, 'test_mse': 0.5123692566496504},\n",
       " {'train_mse': 0.2580305071646758, 'test_mse': 0.5117920377294106},\n",
       " {'train_mse': 0.2574291295108933, 'test_mse': 0.511216546970382},\n",
       " {'train_mse': 0.2568302623031567, 'test_mse': 0.5106427764118581},\n",
       " {'train_mse': 0.2562338911707723, 'test_mse': 0.510070718145701},\n",
       " {'train_mse': 0.2556400018465966, 'test_mse': 0.5095003643158986},\n",
       " {'train_mse': 0.25504858016615034, 'test_mse': 0.5089317071181246},\n",
       " {'train_mse': 0.25445961206674195, 'test_mse': 0.5083647387993041},\n",
       " {'train_mse': 0.25387308358659816, 'test_mse': 0.5077994516571839},\n",
       " {'train_mse': 0.25328898086400425, 'test_mse': 0.5072358380399052},\n",
       " {'train_mse': 0.2527072901364523, 'test_mse': 0.5066738903455827},\n",
       " {'train_mse': 0.2521279977397973, 'test_mse': 0.506113601021886},\n",
       " {'train_mse': 0.2515510901074218, 'test_mse': 0.5055549625656252},\n",
       " {'train_mse': 0.2509765537694087, 'test_mse': 0.5049979675223424},\n",
       " {'train_mse': 0.2504043753517213, 'test_mse': 0.5044426084859048},\n",
       " {'train_mse': 0.24983454157539242, 'test_mse': 0.503888878098103},\n",
       " {'train_mse': 0.24926703925572047, 'test_mse': 0.5033367690482535},\n",
       " {'train_mse': 0.24870185530147335, 'test_mse': 0.5027862740728037},\n",
       " {'train_mse': 0.24813897671410026, 'test_mse': 0.5022373859549422},\n",
       " {'train_mse': 0.24757839058695108, 'test_mse': 0.5016900975242118},\n",
       " {'train_mse': 0.2470200841045028, 'test_mse': 0.501144401656127},\n",
       " {'train_mse': 0.2464640445415939, 'test_mse': 0.5006002912717947},\n",
       " {'train_mse': 0.2459102592626659, 'test_mse': 0.5000577593375387},\n",
       " {'train_mse': 0.2453587157210118, 'test_mse': 0.4995167988645278},\n",
       " {'train_mse': 0.2448094014580322, 'test_mse': 0.4989774029084079},\n",
       " {'train_mse': 0.24426230410249844, 'test_mse': 0.49843956456893607},\n",
       " {'train_mse': 0.24371741136982214, 'test_mse': 0.49790327698962095},\n",
       " {'train_mse': 0.24317471106133293, 'test_mse': 0.4973685333573636},\n",
       " {'train_mse': 0.24263419106356157, 'test_mse': 0.4968353269021039},\n",
       " {'train_mse': 0.24209583934753104, 'test_mse': 0.4963036508964693},\n",
       " {'train_mse': 0.24155964396805396, 'test_mse': 0.49577349865542686},\n",
       " {'train_mse': 0.24102559306303648, 'test_mse': 0.4952448635359397},\n",
       " {'train_mse': 0.24049367485278908, 'test_mse': 0.49471773893662546},\n",
       " {'train_mse': 0.23996387763934365, 'test_mse': 0.4941921182974187},\n",
       " {'train_mse': 0.23943618980577733, 'test_mse': 0.49366799509923603},\n",
       " {'train_mse': 0.23891059981554227, 'test_mse': 0.4931453628636453},\n",
       " {'train_mse': 0.23838709621180218, 'test_mse': 0.4926242151525369},\n",
       " {'train_mse': 0.23786566761677505, 'test_mse': 0.49210454556779903},\n",
       " {'train_mse': 0.23734630273108168, 'test_mse': 0.4915863477509951},\n",
       " {'train_mse': 0.23682899033310073, 'test_mse': 0.49106961538304567},\n",
       " {'train_mse': 0.23631371927832992, 'test_mse': 0.49055434218391125},\n",
       " {'train_mse': 0.23580047849875257, 'test_mse': 0.49004052191228065},\n",
       " {'train_mse': 0.23528925700221118, 'test_mse': 0.4895281483652599},\n",
       " {'train_mse': 0.23478004387178564, 'test_mse': 0.48901721537806564},\n",
       " {'train_mse': 0.23427282826517815, 'test_mse': 0.48850771682372096},\n",
       " {'train_mse': 0.2337675994141036, 'test_mse': 0.48799964661275386},\n",
       " {'train_mse': 0.23326434662368561, 'test_mse': 0.4874929986928987},\n",
       " {'train_mse': 0.2327630592718581, 'test_mse': 0.4869877670488008},\n",
       " {'train_mse': 0.23226372680877305, 'test_mse': 0.48648394570172293},\n",
       " {'train_mse': 0.23176633875621258, 'test_mse': 0.4859815287092561},\n",
       " {'train_mse': 0.23127088470700793, 'test_mse': 0.4854805101650309},\n",
       " {'train_mse': 0.2307773543244628, 'test_mse': 0.48498088419843344},\n",
       " {'train_mse': 0.23028573734178223, 'test_mse': 0.4844826449743232},\n",
       " {'train_mse': 0.22979602356150744, 'test_mse': 0.4839857866927533},\n",
       " {'train_mse': 0.2293082028549549, 'test_mse': 0.48349030358869355},\n",
       " {'train_mse': 0.22882226516166124, 'test_mse': 0.4829961899317565},\n",
       " {'train_mse': 0.22833820048883335, 'test_mse': 0.4825034400259254},\n",
       " {'train_mse': 0.2278559989108032, 'test_mse': 0.48201204820928467},\n",
       " {'train_mse': 0.2273756505684876, 'test_mse': 0.4815220088537542},\n",
       " {'train_mse': 0.22689714566885374, 'test_mse': 0.4810333163648236},\n",
       " {'train_mse': 0.22642047448438868, 'test_mse': 0.4805459651812919},\n",
       " {'train_mse': 0.2259456273525742, 'test_mse': 0.4800599497750066},\n",
       " {'train_mse': 0.22547259467536662, 'test_mse': 0.4795752646506081},\n",
       " {'train_mse': 0.2250013669186807, 'test_mse': 0.4790919043452743},\n",
       " {'train_mse': 0.2245319346118795, 'test_mse': 0.47860986342846823},\n",
       " {'train_mse': 0.2240642883472677, 'test_mse': 0.47812913650168826},\n",
       " {'train_mse': 0.22359841877959, 'test_mse': 0.4776497181982209},\n",
       " {'train_mse': 0.22313431662553437, 'test_mse': 0.47717160318289453},\n",
       " {'train_mse': 0.22267197266323963, 'test_mse': 0.4766947861518368},\n",
       " {'train_mse': 0.2222113777318071, 'test_mse': 0.476219261832234},\n",
       " {'train_mse': 0.2217525227308178, 'test_mse': 0.47574502498209154},\n",
       " {'train_mse': 0.22129539861985273, 'test_mse': 0.475272070389998},\n",
       " {'train_mse': 0.22083999641801838, 'test_mse': 0.47480039287489073},\n",
       " {'train_mse': 0.2203863072034764, 'test_mse': 0.4743299872858237},\n",
       " {'train_mse': 0.21993432211297745, 'test_mse': 0.4738608485017371},\n",
       " {'train_mse': 0.2194840323413988, 'test_mse': 0.47339297143122916},\n",
       " {'train_mse': 0.21903542914128746, 'test_mse': 0.47292635101233116},\n",
       " {'train_mse': 0.21858850382240586, 'test_mse': 0.4724609822122828},\n",
       " {'train_mse': 0.2181432477512827, 'test_mse': 0.47199686002731006},\n",
       " {'train_mse': 0.21769965235076735, 'test_mse': 0.4715339794824064},\n",
       " {'train_mse': 0.2172577090995887, 'test_mse': 0.4710723356311138},\n",
       " {'train_mse': 0.21681740953191736, 'test_mse': 0.47061192355530806},\n",
       " {'train_mse': 0.21637874523693243, 'test_mse': 0.4701527383649835},\n",
       " {'train_mse': 0.21594170785839187, 'test_mse': 0.4696947751980423},\n",
       " {'train_mse': 0.2155062890942066, 'test_mse': 0.4692380292200835},\n",
       " {'train_mse': 0.215072480696019, 'test_mse': 0.46878249562419533},\n",
       " {'train_mse': 0.2146402744687841, 'test_mse': 0.4683281696307484},\n",
       " {'train_mse': 0.21420966227035573, 'test_mse': 0.467875046487192},\n",
       " {'train_mse': 0.21378063601107594, 'test_mse': 0.4674231214678509},\n",
       " {'train_mse': 0.21335318765336742, 'test_mse': 0.4669723898737244},\n",
       " {'train_mse': 0.21292730921133066, 'test_mse': 0.46652284703228786},\n",
       " {'train_mse': 0.21250299275034384, 'test_mse': 0.4660744882972953},\n",
       " {'train_mse': 0.21208023038666687, 'test_mse': 0.4656273090485837},\n",
       " {'train_mse': 0.21165901428704847, 'test_mse': 0.4651813046918795},\n",
       " {'train_mse': 0.21123933666833722, 'test_mse': 0.4647364706586064},\n",
       " {'train_mse': 0.21082118979709571, 'test_mse': 0.4642928024056956},\n",
       " {'train_mse': 0.21040456598921806, 'test_mse': 0.46385029541539635},\n",
       " {'train_mse': 0.20998945760955137, 'test_mse': 0.46340894519509},\n",
       " {'train_mse': 0.2095758570715198, 'test_mse': 0.46296874727710396},\n",
       " {'train_mse': 0.20916375683675256, 'test_mse': 0.46252969721852877},\n",
       " {'train_mse': 0.20875314941471482, 'test_mse': 0.4620917906010353},\n",
       " {'train_mse': 0.20834402736234214, 'test_mse': 0.461655023030695},\n",
       " {'train_mse': 0.2079363832836778, 'test_mse': 0.46121939013780133},\n",
       " {'train_mse': 0.20753020982951387, 'test_mse': 0.4607848875766916},\n",
       " {'train_mse': 0.2071254996970346, 'test_mse': 0.4603515110255724},\n",
       " {'train_mse': 0.20672224562946384, 'test_mse': 0.459919256186345},\n",
       " {'train_mse': 0.20632044041571512, 'test_mse': 0.45948811878443235},\n",
       " {'train_mse': 0.2059200768900446, 'test_mse': 0.4590580945686089},\n",
       " {'train_mse': 0.20552114793170753, 'test_mse': 0.45862917931083075},\n",
       " {'train_mse': 0.20512364646461737, 'test_mse': 0.45820136880606754},\n",
       " {'train_mse': 0.204727565457008, 'test_mse': 0.45777465887213525},\n",
       " {'train_mse': 0.204332897921099, 'test_mse': 0.4573490453495325},\n",
       " {'train_mse': 0.20393963691276354, 'test_mse': 0.45692452410127543},\n",
       " {'train_mse': 0.20354777553119957, 'test_mse': 0.45650109101273634}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
