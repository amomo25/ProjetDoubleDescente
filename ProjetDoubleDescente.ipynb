{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.76033058e-06  1.30472388e-05  1.64211717e-05 ...  4.26304984e-06\n",
      " -2.82637457e-05  2.12524363e-06]\n",
      "[3.16227766 3.16227766 3.16227766 3.16227766 3.16227766 3.16227766\n",
      " 3.16227766 3.16227766 3.16227766 3.16227766]\n"
     ]
    }
   ],
   "source": [
    "d = 10\n",
    "n_train = 10000\n",
    "n_test = 100\n",
    "\n",
    "x_train = np.random.randn(n_train, d)\n",
    "x_test = np.random.randn(n_test, d)\n",
    "mean_train = np.empty(d)\n",
    "norm_tab = np.empty(n_train)\n",
    "\n",
    "norm_train = np.sum(x_train * x_train, axis = 0) ** 0.5\n",
    "x_train = (x_train / norm_train) * (d ** 0.5) \n",
    "    \n",
    "norm_test = np.sum(x_test * x_test, axis = 0) **0.5\n",
    "x_test = (x_test / norm_test) * (d**0.5)\n",
    "\n",
    "#On vérifie que notre data set soit uniforme\n",
    "mean_train = np.sum(x_train, axis = 1) / n\n",
    "    \n",
    "#On vérifie que notre dataset ait une norme de sqrt(d)\n",
    "norm_tab = np.sum(x_train * x_train, axis = 0) ** 0.5\n",
    "    \n",
    "print(mean_train)\n",
    "print(norm_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01805749]\n",
      " [-0.06229601]\n",
      " [ 0.06265033]\n",
      " ...\n",
      " [-0.02420928]\n",
      " [ 0.11474633]\n",
      " [-0.01064783]]\n"
     ]
    }
   ],
   "source": [
    "tau = 0.3 \n",
    "noise_level = tau * tau\n",
    "\n",
    "#On veut E(noise) == 0 et E(noise^2) == tau^2\n",
    "noise_train = np.random.randn(n_train, 1) * noise_level\n",
    "noise_test = np.random.randn(n_test, 1) * noise_level\n",
    "\n",
    "print(noise_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.63042882]\n",
      " [ 0.02091969]\n",
      " [-0.25413631]\n",
      " [ 0.18569001]\n",
      " [ 0.31498142]\n",
      " [ 0.14747567]\n",
      " [-0.52809966]\n",
      " [-0.00218809]\n",
      " [-0.32123861]\n",
      " [ 0.00227512]]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "F1 = 1\n",
    "\n",
    "sample_params = np.random.randn(d, 1)\n",
    "    \n",
    "norm = np.sum(sample_params * sample_params) #should be equal to F1^2\n",
    "sample_params = sample_params * (F1 / (norm**0.5))\n",
    "\n",
    "print(sample_params)\n",
    "print(np.sum(sample_params * sample_params) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.50890513]\n",
      " [-2.04905717]\n",
      " [-0.22448405]\n",
      " ...\n",
      " [ 0.27923782]\n",
      " [ 0.28757629]\n",
      " [-0.38532663]]\n"
     ]
    }
   ],
   "source": [
    "y_train = x_train @ sample_params + noise_train\n",
    "y_test = x_test @ sample_params + noise_test\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques fonctions d'activation :\n",
    "\n",
    "class Sigmoid :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        s = Sigmoid.function(x)\n",
    "        return s * (1- s)\n",
    "    \n",
    "class Tanh :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        t = Tanh(x)\n",
    "        return 1-t**2\n",
    "    \n",
    "class Relu :\n",
    "    @staticmethod\n",
    "    def function(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(x):\n",
    "        return np.maximum(0, x) / x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quelques Loss Functions :\n",
    "class MSE:\n",
    "    @staticmethod\n",
    "    def loss(y_real, y_hat):\n",
    "        return (1/(y_real.shape[0] * y_real.shape[1])) * np.sum(np.sum((y_hat - y_real)**2, axis = 0))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y_real, y_hat):\n",
    "        return (2/(y_real.shape[0] * y_real.shape[1])) * (y_hat - y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'architecture\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, dimension_hidden, activation1, activation2):\n",
    "        \"\"\"\n",
    "        dimension_hidden est le nombre de paramètres dans le hidden layer (N dans le papier de Mei et Montanari)\n",
    "        activation1 est la fonction d'activation du hidden layer\n",
    "        activation2 est la fonction d'activation de l'output layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.nb_layers = 3 #input, hidden, output\n",
    "        self.dimensions = (d, dimension_hidden, 1)\n",
    "        \n",
    "        self.loss_tab = None\n",
    "        \n",
    "        self.learning_rate = {}\n",
    "        self.learning_rate[1] = None;  #learning rate du hidden layer\n",
    "        self.learning_rate[2] = None;  #learning rate du output layer\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        \n",
    "        #on initialise les weights et les bias aléatoirement\n",
    "        for i in range(self.nb_layers -1):\n",
    "            self.weights[i + 1] = np.random.randn(self.dimensions[i], self.dimensions[i + 1]) / np.sqrt(self.dimensions[i])\n",
    "            self.bias[i + 1] = np.zeros(self.dimensions[i + 1])\n",
    "            \n",
    "        self.activations = {}\n",
    "        self.activations[2] = activation1\n",
    "        self.activations[3] = activation2\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "        x est un vecteur de notre data\n",
    "        \"\"\"\n",
    "        z = {}\n",
    "        a = {1:x} #l'input layer n'a pas d'activation function, a[1] est donc égal à x\n",
    "        \n",
    "        for i in range(1, self.nb_layers):\n",
    "            z[i + 1] = a[i] @ self.weights[i] + self.bias[i] #Z = XW + b\n",
    "            a[i + 1] = self.activations[i + 1].function(z[i + 1]);\n",
    "            \n",
    "        return z, a\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z, a = self.forward_pass(x)\n",
    "        return a[self.nb_layers]\n",
    "    \n",
    "    def back_propagation(self, z, a, real_value):\n",
    "        y_hat = a[self.nb_layers]\n",
    "        \n",
    "        #On calcule delta et la dérivée partielle à l'output layer\n",
    "        delta = self.loss_function.gradient(real_value, y_hat) * self.activations[self.nb_layers].gradient(y_hat)\n",
    "        partial_deriv = a[self.nb_layers - 1].T @ delta\n",
    "        \n",
    "        update_parameters = {\n",
    "            self.nb_layers - 1: (partial_deriv, delta)\n",
    "        }\n",
    "        \n",
    "        for i in reversed(range(2, self.nb_layers)):\n",
    "            delta = (delta @ self.weights[i].T) * self.activations[i].gradient(z[i])\n",
    "            partial_deriv = a[i - 1].T @ delta \n",
    "            update_parameters[i - 1] = (partial_deriv, delta)\n",
    "            \n",
    "        for k, v in update_parameters.items():\n",
    "            self.update_weights_and_bias(k, v[0], v[1])\n",
    "            \n",
    "    def update_weights_and_bias(self, index, partial_deriv, delta):\n",
    "        self.weights[index] -= self.learning_rate[index] * partial_deriv\n",
    "        self.bias[index] -= self.learning_rate[index] * np.mean(delta, 0)\n",
    "        \n",
    "    def fit(self, x, y_real, loss, nb_iterations, batch_size, learning_rate1, learning_rate2):\n",
    "        #On vérifie qu'on a autant de x que de y\n",
    "        if not (x.shape[0] == y_real.shape[0]):\n",
    "            raise Exception\n",
    "            \n",
    "        self.loss_tab = np.empty((nb_iterations, x.shape[0] // batch_size))\n",
    "        \n",
    "        self.loss_function = loss\n",
    "        self.learning_rate[1] = learning_rate1\n",
    "        self.learning_rate[2] = learning_rate2\n",
    "        \n",
    "        #We use batch gradient descent\n",
    "        for i in range(nb_iterations):\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                z, a = self.forward_pass(x[start:end])\n",
    "                self.loss_tab[i][j] = self.loss_function.loss(y_real[start:end], a[self.nb_layers])\n",
    "                self.back_propagation(z, a, y_real[start:end])\n",
    "            if(i % 10) == 0:\n",
    "                print(\"Loss at Iteration {} is {}\".format(i, self.loss_tab[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at Iteration 0 is [0.26552766 0.25932027 0.25860809 0.25182894 0.24571689 0.24615723\n",
      " 0.24564879 0.24334162 0.24637182 0.23718969]\n",
      "Loss at Iteration 10 is [0.1000337  0.09896867 0.09802594 0.09522092 0.09394852 0.09370557\n",
      " 0.09398595 0.09355521 0.09704572 0.09218913]\n",
      "Loss at Iteration 20 is [0.05170545 0.05171131 0.05100437 0.04992534 0.04931549 0.04916219\n",
      " 0.04963825 0.04955048 0.05263307 0.04916221]\n",
      "Loss at Iteration 30 is [0.03360323 0.03382852 0.03326525 0.03299304 0.03239684 0.03231616\n",
      " 0.03284391 0.03283458 0.03546886 0.03259338]\n",
      "Loss at Iteration 40 is [0.02508122 0.02534266 0.02486698 0.0250338  0.02437728 0.0243341\n",
      " 0.02487216 0.02489537 0.02717507 0.02463399]\n",
      "Loss at Iteration 50 is [0.02040193 0.02065293 0.02023561 0.02066846 0.01995979 0.01993453\n",
      " 0.02046732 0.02051207 0.02252125 0.02019877]\n",
      "Loss at Iteration 60 is [0.01754769 0.01777621 0.01740059 0.01800728 0.01726257 0.01724488\n",
      " 0.01776608 0.01782837 0.01962872 0.01746237]\n",
      "Loss at Iteration 70 is [0.01567077 0.01587486 0.01553062 0.01625735 0.01548949 0.01547381\n",
      " 0.01598101 0.01605863 0.01769444 0.01564602]\n",
      "Loss at Iteration 80 is [0.01436547 0.01454628 0.01422656 0.01503971 0.01425783 0.0142411\n",
      " 0.01473366 0.01482499 0.01632846 0.01437265]\n",
      "Loss at Iteration 90 is [0.01341768 0.01357724 0.01327726 0.01415469 0.013365   0.01334555\n",
      " 0.01382366 0.01392738 0.01532242 0.01344143]\n",
      "Loss at Iteration 100 is [0.01270556 0.012846   0.01256228 0.0134888  0.01269551 0.01267243\n",
      " 0.01313666 0.01325165 0.01455641 0.01273719]\n",
      "0.11754059233394026\n"
     ]
    }
   ],
   "source": [
    "nn = Network(10000, Relu, Sigmoid)\n",
    "nn.fit(x_train, y_train, MSE, 101, 1000, 0, 0.03)\n",
    "\n",
    "z, a = nn.forward_pass(x_test)\n",
    "print(nn.loss_function.loss(y_test, a[nn.nb_layers]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
